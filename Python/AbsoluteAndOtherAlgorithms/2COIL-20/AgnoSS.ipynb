{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='./Dataset/coil-20-proc/'\n",
    "\n",
    "samples={}\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
    "    #print(dirpath)\n",
    "    #print(dirnames)\n",
    "    #print(filenames)\n",
    "    dirnames.sort()\n",
    "    filenames.sort()\n",
    "    for filename in [f for f in filenames if f.endswith(\".png\") and not f.find('checkpoint')>0]:\n",
    "        full_path = os.path.join(dirpath, filename)\n",
    "        file_identifier=filename.split('__')[0][3:]\n",
    "        if file_identifier not in samples.keys():\n",
    "            samples[file_identifier] = []\n",
    "        # Direct read\n",
    "        #image = io.imread(full_path)\n",
    "        # Resize read\n",
    "        image_=Image.open(full_path).resize((20, 20),Image.ANTIALIAS)\n",
    "        image=np.asarray(image_)\n",
    "        samples[file_identifier].append(image)\n",
    "        \n",
    "#plt.imshow(samples['1'][0].reshape(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr_list=[]\n",
    "label_arr_list=[]\n",
    "for key_i in samples.keys():\n",
    "    key_i_for_label=[int(key_i)-1]\n",
    "    data_arr_list.append(np.array(samples[key_i]))\n",
    "    label_arr_list.append(np.array(72*key_i_for_label))\n",
    "    \n",
    "data_arr=np.concatenate(data_arr_list).reshape(1440, 20*20).astype('float32') / 255.\n",
    "label_arr_onehot=np.concatenate(label_arr_list)#to_categorical(np.concatenate(label_arr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (1036, 400)\n",
      "Shape of x_validate: (116, 400)\n",
      "Shape of x_test: (288, 400)\n",
      "Shape of y_train: (1036,)\n",
      "Shape of y_validate: (116,)\n",
      "Shape of y_test: (288,)\n",
      "Shape of C_train_x: (1152, 400)\n",
      "Shape of C_train_y: (1152,)\n",
      "Shape of C_test_x: (288, 400)\n",
      "Shape of C_test_y: (288,)\n"
     ]
    }
   ],
   "source": [
    "C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(data_arr,label_arr_onehot,test_size=0.2,random_state=seed)\n",
    "x_train,x_validate,y_train_onehot,y_validate_onehot= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=seed)\n",
    "x_test=C_test_x\n",
    "y_test_onehot=C_test_y\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_validate: ' + str(x_validate.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train_onehot.shape))\n",
    "print('Shape of y_validate: ' + str(y_validate_onehot.shape))\n",
    "print('Shape of y_test: ' + str(y_test_onehot.shape))\n",
    "\n",
    "print('Shape of C_train_x: ' + str(C_train_x.shape)) \n",
    "print('Shape of C_train_y: ' + str(C_train_y.shape)) \n",
    "print('Shape of C_test_x: ' + str(C_test_x.shape)) \n",
    "print('Shape of C_test_y: ' + str(C_test_y.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "class Feature_Select_Layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, l1_lambda, **kwargs):\n",
    "        super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.l1_lambda=l1_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0., maxval=1.),\n",
    "                                      trainable=True,\n",
    "                                      regularizer=regularizers.l1(self.l1_lambda),\n",
    "                                      constraint=constraints.NonNeg())\n",
    "        super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, selection=False,k=36):\n",
    "        kernel=self.kernel        \n",
    "        if selection:\n",
    "            kernel_=K.transpose(kernel)\n",
    "            print(kernel_.shape)\n",
    "            kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "            kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "        return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                         p_encoding_dim=50,\\\n",
    "                         p_learning_rate= 1E-3,\\\n",
    "                         p_l1_lambda=0.1):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             l1_lambda=p_l1_lambda,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,latent_encoder_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "batch_size_value=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1.1 Identity Autoencoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Autoencoder Structure-------------------------------------\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "feature_selection (Feature_S (None, 400)               400       \n",
      "_________________________________________________________________\n",
      "autoencoder_hidden_layer (De (None, 50)                20050     \n",
      "_________________________________________________________________\n",
      "autoencoder_output (Dense)   (None, 400)               20400     \n",
      "=================================================================\n",
      "Total params: 40,850\n",
      "Trainable params: 40,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ide_AE,\\\n",
    "latent_encoder_score_Ide_AE=Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                                 p_encoding_dim=50,\\\n",
    "                                                 p_learning_rate= 1E-2,\\\n",
    "                                                 p_l1_lambda=l1_lambda)\n",
    "\n",
    "file_name=\"./log/AgnoSS.png\"\n",
    "plot_model(Ide_AE, to_file=file_name,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 1036 samples, validate on 116 samples\n",
      "Epoch 1/1000\n",
      "1036/1036 [==============================] - 0s 382us/step - loss: 183.1081 - val_loss: 163.1995\n",
      "Epoch 2/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 150.5881 - val_loss: 131.6940\n",
      "Epoch 3/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 120.2040 - val_loss: 103.2209\n",
      "Epoch 4/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 92.9570 - val_loss: 78.1554\n",
      "Epoch 5/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 69.3752 - val_loss: 56.8468\n",
      "Epoch 6/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 49.6423 - val_loss: 39.2589\n",
      "Epoch 7/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 33.2466 - val_loss: 24.7093\n",
      "Epoch 8/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 20.1393 - val_loss: 13.7876\n",
      "Epoch 9/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 10.5240 - val_loss: 6.2426\n",
      "Epoch 10/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 4.3524 - val_loss: 1.9745\n",
      "Epoch 11/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.9605 - val_loss: 0.0843\n",
      "Epoch 12/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0644 - val_loss: 0.0605\n",
      "Epoch 13/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 14/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 15/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0615 - val_loss: 0.0612\n",
      "Epoch 16/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0616 - val_loss: 0.0612\n",
      "Epoch 17/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 18/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 19/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 20/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 21/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 22/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 23/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0614 - val_loss: 0.0612\n",
      "Epoch 24/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0618 - val_loss: 0.0604\n",
      "Epoch 25/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 26/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0615 - val_loss: 0.0601\n",
      "Epoch 27/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0616 - val_loss: 0.0607\n",
      "Epoch 28/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 29/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 30/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 31/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 32/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 33/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 34/1000\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 0.0615 - val_loss: 0.0609\n",
      "Epoch 35/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 36/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0613 - val_loss: 0.0610\n",
      "Epoch 37/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0615 - val_loss: 0.0602\n",
      "Epoch 38/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 39/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 40/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0617 - val_loss: 0.0602\n",
      "Epoch 41/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0604\n",
      "Epoch 42/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 43/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0618 - val_loss: 0.0615\n",
      "Epoch 44/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0616 - val_loss: 0.0600\n",
      "Epoch 45/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0618 - val_loss: 0.0624\n",
      "Epoch 46/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0620 - val_loss: 0.0601\n",
      "Epoch 47/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0615 - val_loss: 0.0612\n",
      "Epoch 48/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 49/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0610\n",
      "Epoch 50/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0615 - val_loss: 0.0607\n",
      "Epoch 51/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 52/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 53/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0616 - val_loss: 0.0606\n",
      "Epoch 54/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0616 - val_loss: 0.0612\n",
      "Epoch 55/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 56/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 57/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 58/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 59/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0614 - val_loss: 0.0613\n",
      "Epoch 60/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0617 - val_loss: 0.0604\n",
      "Epoch 61/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0613\n",
      "Epoch 62/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0616 - val_loss: 0.0604\n",
      "Epoch 63/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 64/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 65/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0616 - val_loss: 0.0605\n",
      "Epoch 66/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0610\n",
      "Epoch 67/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 68/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 69/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0616 - val_loss: 0.0606\n",
      "Epoch 70/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0617 - val_loss: 0.0609\n",
      "Epoch 71/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 72/1000\n",
      "1036/1036 [==============================] - 0s 71us/step - loss: 0.0616 - val_loss: 0.0616\n",
      "Epoch 73/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0621 - val_loss: 0.0602\n",
      "Epoch 74/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0616 - val_loss: 0.0606\n",
      "Epoch 75/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0610\n",
      "Epoch 76/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 77/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0613\n",
      "Epoch 78/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0603\n",
      "Epoch 79/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0617 - val_loss: 0.0611\n",
      "Epoch 80/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 81/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0609\n",
      "Epoch 82/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 83/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 84/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0604\n",
      "Epoch 85/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0617 - val_loss: 0.0607\n",
      "Epoch 86/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 87/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 88/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 89/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0616 - val_loss: 0.0610\n",
      "Epoch 90/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0613\n",
      "Epoch 91/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0616 - val_loss: 0.0601\n",
      "Epoch 92/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0620 - val_loss: 0.0618\n",
      "Epoch 93/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0621 - val_loss: 0.0604\n",
      "Epoch 94/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0616 - val_loss: 0.0613\n",
      "Epoch 95/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0601\n",
      "Epoch 96/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 97/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 98/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0613\n",
      "Epoch 99/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 100/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "\n",
      "Epoch 00100: saving model to ./log_weights/Ide_AE_weights.0100.hdf5\n",
      "Epoch 101/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 102/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 103/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0607\n",
      "Epoch 104/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0607\n",
      "Epoch 105/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0604\n",
      "Epoch 106/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 107/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0613\n",
      "Epoch 108/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0617 - val_loss: 0.0605\n",
      "Epoch 109/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0615 - val_loss: 0.0613\n",
      "Epoch 110/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0617 - val_loss: 0.0605\n",
      "Epoch 111/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0617 - val_loss: 0.0608\n",
      "Epoch 112/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0615 - val_loss: 0.0607\n",
      "Epoch 113/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 114/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0602\n",
      "Epoch 115/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 116/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 117/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 118/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 119/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0615 - val_loss: 0.0602\n",
      "Epoch 120/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0616 - val_loss: 0.0607\n",
      "Epoch 121/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 122/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 123/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 124/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 125/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 126/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 127/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 128/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0609\n",
      "Epoch 129/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 130/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0617\n",
      "Epoch 131/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0617 - val_loss: 0.0606\n",
      "Epoch 132/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0614 - val_loss: 0.0609\n",
      "Epoch 133/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0614 - val_loss: 0.0615\n",
      "Epoch 134/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0616 - val_loss: 0.0603\n",
      "Epoch 135/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0618 - val_loss: 0.0607\n",
      "Epoch 136/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 137/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0616 - val_loss: 0.0611\n",
      "Epoch 138/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0616 - val_loss: 0.0611\n",
      "Epoch 139/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 140/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 141/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 142/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 143/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 144/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0616\n",
      "Epoch 145/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0616 - val_loss: 0.0608\n",
      "Epoch 146/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0610\n",
      "Epoch 147/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 148/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 149/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 150/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 151/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 152/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0612\n",
      "Epoch 153/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 154/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 155/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 156/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0615 - val_loss: 0.0603\n",
      "Epoch 157/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0615 - val_loss: 0.0609\n",
      "Epoch 158/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0601\n",
      "Epoch 159/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0618 - val_loss: 0.0610\n",
      "Epoch 160/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 161/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0612\n",
      "Epoch 162/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0615 - val_loss: 0.0609\n",
      "Epoch 163/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0614 - val_loss: 0.0610\n",
      "Epoch 164/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0616 - val_loss: 0.0606\n",
      "Epoch 165/1000\n",
      "1036/1036 [==============================] - 0s 73us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 166/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 167/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 168/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 169/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 170/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 171/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 172/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 173/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 174/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 175/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 176/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 177/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0612\n",
      "Epoch 178/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 179/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 180/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 181/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 182/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 183/1000\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 184/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 185/1000\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 186/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 187/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0616 - val_loss: 0.0606\n",
      "Epoch 188/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 189/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 190/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 191/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 192/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 193/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 194/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0615 - val_loss: 0.0614\n",
      "Epoch 195/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0615 - val_loss: 0.0602\n",
      "Epoch 196/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 197/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 198/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 199/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 200/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "\n",
      "Epoch 00200: saving model to ./log_weights/Ide_AE_weights.0200.hdf5\n",
      "Epoch 201/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 202/1000\n",
      "1036/1036 [==============================] - 0s 74us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 203/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 204/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 205/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 206/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 207/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 208/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 209/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0614 - val_loss: 0.0602\n",
      "Epoch 210/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0610\n",
      "Epoch 211/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 212/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 213/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 214/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0611\n",
      "Epoch 215/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0611\n",
      "Epoch 216/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 217/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 218/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0610\n",
      "Epoch 219/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 220/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 221/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0613 - val_loss: 0.0610\n",
      "Epoch 222/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 223/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 224/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 225/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 226/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0600\n",
      "Epoch 227/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 228/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 229/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0610\n",
      "Epoch 230/1000\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 0.0614 - val_loss: 0.0611\n",
      "Epoch 231/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 232/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 233/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 234/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 235/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 236/1000\n",
      "1036/1036 [==============================] - 0s 75us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 237/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 238/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 239/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 240/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0613 - val_loss: 0.0614\n",
      "Epoch 241/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0615 - val_loss: 0.0610\n",
      "Epoch 242/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 243/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 244/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 245/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 246/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 247/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 248/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 249/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 250/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0614 - val_loss: 0.0606\n",
      "Epoch 251/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 252/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0615 - val_loss: 0.0610\n",
      "Epoch 253/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 254/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 255/1000\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 256/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 257/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0611\n",
      "Epoch 258/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0605\n",
      "Epoch 259/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0610\n",
      "Epoch 260/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0614\n",
      "Epoch 261/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 262/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 263/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 264/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 265/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 266/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0615 - val_loss: 0.0611\n",
      "Epoch 267/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 268/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 269/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 270/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 271/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 272/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0615 - val_loss: 0.0608\n",
      "Epoch 273/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 274/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 275/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 276/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 277/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 278/1000\n",
      "1036/1036 [==============================] - 0s 81us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 279/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0612 - val_loss: 0.0611\n",
      "Epoch 280/1000\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 281/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0601\n",
      "Epoch 282/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0614 - val_loss: 0.0608\n",
      "Epoch 283/1000\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 284/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 285/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0611\n",
      "Epoch 286/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 287/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 288/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0613 - val_loss: 0.0611\n",
      "Epoch 289/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 290/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 291/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 292/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 293/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 294/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 295/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 296/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 297/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0612 - val_loss: 0.0611\n",
      "Epoch 298/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 299/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0614 - val_loss: 0.0602\n",
      "Epoch 300/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "\n",
      "Epoch 00300: saving model to ./log_weights/Ide_AE_weights.0300.hdf5\n",
      "Epoch 301/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 302/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0613 - val_loss: 0.0611\n",
      "Epoch 303/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 304/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 305/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 306/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 307/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 308/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 309/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 310/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0610\n",
      "Epoch 311/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 312/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 313/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 314/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 315/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 316/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 317/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 318/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 319/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 320/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 321/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 322/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0611\n",
      "Epoch 323/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 324/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0614 - val_loss: 0.0609\n",
      "Epoch 325/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 326/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 327/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0610\n",
      "Epoch 328/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 329/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 330/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 331/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 332/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 333/1000\n",
      "1036/1036 [==============================] - 0s 75us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 334/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 335/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 336/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 337/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 338/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 339/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0613 - val_loss: 0.0601\n",
      "Epoch 340/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 341/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 342/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 343/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 344/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 345/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 346/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 347/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 348/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 349/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 350/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 351/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 352/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0609\n",
      "Epoch 353/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 354/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 355/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 356/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 357/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 358/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 359/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 360/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 361/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 362/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 363/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 364/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 365/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 366/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0611\n",
      "Epoch 367/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 368/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 369/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 370/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 371/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 372/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 373/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 374/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 375/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 376/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 377/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 378/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 379/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 380/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0610\n",
      "Epoch 381/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 382/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 383/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 384/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 385/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 386/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 387/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 388/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 389/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 390/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 391/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 392/1000\n",
      "1036/1036 [==============================] - 0s 74us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 393/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 394/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 395/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 396/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 397/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 398/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 399/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0615\n",
      "Epoch 400/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "\n",
      "Epoch 00400: saving model to ./log_weights/Ide_AE_weights.0400.hdf5\n",
      "Epoch 401/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0601\n",
      "Epoch 402/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 403/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 404/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 405/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 406/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 407/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 408/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 409/1000\n",
      "1036/1036 [==============================] - 0s 76us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 410/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 411/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 412/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 413/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 414/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 415/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 416/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 417/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 418/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 419/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 420/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 421/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 422/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 423/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 424/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 425/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 426/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0613 - val_loss: 0.0609\n",
      "Epoch 427/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 428/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0609\n",
      "Epoch 429/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 430/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 431/1000\n",
      "1036/1036 [==============================] - 0s 74us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 432/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 433/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 434/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 435/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 436/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 437/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 438/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 439/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 440/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 441/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 442/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 443/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 444/1000\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 445/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 446/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 447/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 448/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 449/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 450/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 451/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 452/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 453/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 454/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 455/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 456/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 457/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 458/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 459/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 460/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 461/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 462/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 463/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 464/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 465/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 466/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 467/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 468/1000\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 469/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 470/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 471/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0601\n",
      "Epoch 472/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 473/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 474/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 475/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 476/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 477/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 478/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 479/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 480/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0601\n",
      "Epoch 481/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 482/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0612\n",
      "Epoch 483/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 484/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 485/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 486/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 487/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 488/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 489/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 490/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 491/1000\n",
      "1036/1036 [==============================] - 0s 72us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 492/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 493/1000\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 494/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 495/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 496/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 497/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 498/1000\n",
      "1036/1036 [==============================] - ETA: 0s - loss: 0.060 - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 499/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 500/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "\n",
      "Epoch 00500: saving model to ./log_weights/Ide_AE_weights.0500.hdf5\n",
      "Epoch 501/1000\n",
      "1036/1036 [==============================] - ETA: 0s - loss: 0.060 - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 502/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 503/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 504/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 505/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 506/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 507/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 508/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 509/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 510/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 511/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 512/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 513/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 514/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0611 - val_loss: 0.0609\n",
      "Epoch 515/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 516/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 517/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 518/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 519/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 520/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 521/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 522/1000\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 523/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 524/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 525/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 526/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 527/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 528/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 529/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 530/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 531/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 532/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 533/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 534/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 535/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 536/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 537/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 538/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 539/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 540/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 541/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 542/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 543/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 544/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 545/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 546/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 547/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 548/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 549/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 550/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 551/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 552/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 553/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 554/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 555/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 556/1000\n",
      "1036/1036 [==============================] - 0s 82us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 557/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 558/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 559/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 560/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 561/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 562/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 563/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 564/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 565/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 566/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 567/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 568/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 569/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 570/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 571/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 572/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 573/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 574/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 575/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 576/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 577/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 578/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 579/1000\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 580/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 581/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 582/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 583/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 584/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 585/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 586/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 587/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 588/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 589/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 590/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 591/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 592/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 593/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 594/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 595/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 596/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 597/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 598/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 599/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 600/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "\n",
      "Epoch 00600: saving model to ./log_weights/Ide_AE_weights.0600.hdf5\n",
      "Epoch 601/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 602/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 603/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 604/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 605/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0612 - val_loss: 0.0601\n",
      "Epoch 606/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 607/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 608/1000\n",
      "1036/1036 [==============================] - 0s 75us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 609/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 610/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 611/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 612/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 613/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 614/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 615/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 616/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 617/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 618/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 619/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 620/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 621/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 622/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 623/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 624/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 625/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 626/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 627/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0601\n",
      "Epoch 628/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 629/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 630/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 631/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 632/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 633/1000\n",
      "1036/1036 [==============================] - 0s 72us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 634/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 635/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 636/1000\n",
      "1036/1036 [==============================] - ETA: 0s - loss: 0.061 - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 637/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 638/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 639/1000\n",
      "1036/1036 [==============================] - 0s 72us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 640/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 641/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 642/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 643/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 644/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 645/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 646/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 647/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 648/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 649/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 650/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 651/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 652/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 653/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 654/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 655/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 656/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 657/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 658/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 659/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 660/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 661/1000\n",
      "1036/1036 [==============================] - 0s 81us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 662/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 663/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 664/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 665/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 666/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 667/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 668/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 669/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 670/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 671/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 672/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 673/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 674/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 675/1000\n",
      "1036/1036 [==============================] - 0s 47us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 676/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 677/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 678/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 679/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 680/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 681/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 682/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 683/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 684/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 685/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 686/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 687/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 688/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 689/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 690/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 691/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 692/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 693/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 694/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 695/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 696/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 697/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 698/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 699/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 700/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "\n",
      "Epoch 00700: saving model to ./log_weights/Ide_AE_weights.0700.hdf5\n",
      "Epoch 701/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 702/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 703/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 704/1000\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 705/1000\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 706/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 707/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 708/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 709/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 710/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 711/1000\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 712/1000\n",
      "1036/1036 [==============================] - 0s 45us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 713/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 714/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 715/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 716/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 717/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 718/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 719/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 720/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 721/1000\n",
      "1036/1036 [==============================] - 0s 77us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 722/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 723/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 724/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 725/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 726/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 727/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 728/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 729/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 730/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 731/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 732/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 733/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 734/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 735/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 736/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 737/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 738/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 739/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 740/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 741/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 742/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 743/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0608\n",
      "Epoch 744/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 745/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 746/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 747/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 748/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 749/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 750/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 751/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 752/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 753/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 754/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0611\n",
      "Epoch 755/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 756/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 757/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 758/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 759/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 760/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 761/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 762/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 763/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 764/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 765/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 766/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 767/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 768/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 769/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0613 - val_loss: 0.0602\n",
      "Epoch 770/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0611\n",
      "Epoch 771/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 772/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 773/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 774/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 775/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 776/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 777/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 778/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 779/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 780/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 781/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 782/1000\n",
      "1036/1036 [==============================] - 0s 48us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 783/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 784/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 785/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 786/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 787/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 788/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 789/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 790/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 791/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 792/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 793/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 794/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 795/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 796/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 797/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 798/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 799/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 800/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "\n",
      "Epoch 00800: saving model to ./log_weights/Ide_AE_weights.0800.hdf5\n",
      "Epoch 801/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 802/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 803/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 804/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 805/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 806/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 807/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 808/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 809/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 810/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 811/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 812/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 813/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 814/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 815/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 816/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 817/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 818/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 819/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0602\n",
      "Epoch 820/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 821/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 822/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 823/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 824/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 825/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 826/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 827/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 828/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 829/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 830/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 831/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 832/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 833/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 834/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 835/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 836/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 837/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 838/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 839/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 840/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 841/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 842/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 843/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 844/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 845/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 846/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 847/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 848/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 849/1000\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 850/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 851/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 852/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 853/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 854/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 855/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 856/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 857/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 858/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 859/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 860/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 861/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 862/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 863/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 864/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 865/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 866/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 867/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 868/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 869/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 870/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 871/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 872/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 873/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 874/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0602\n",
      "Epoch 875/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 876/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 877/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 878/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 879/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 880/1000\n",
      "1036/1036 [==============================] - 0s 52us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 881/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 882/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 883/1000\n",
      "1036/1036 [==============================] - 0s 77us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 884/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 885/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 886/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 887/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 888/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 889/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0613 - val_loss: 0.0607\n",
      "Epoch 890/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0614 - val_loss: 0.0604\n",
      "Epoch 891/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 892/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 893/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 894/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 895/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0613 - val_loss: 0.0608\n",
      "Epoch 896/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 897/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 898/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 899/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 900/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "\n",
      "Epoch 00900: saving model to ./log_weights/Ide_AE_weights.0900.hdf5\n",
      "Epoch 901/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 902/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 903/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 904/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 905/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 906/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 907/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 908/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 909/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 910/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 911/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 912/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 913/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 914/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 915/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 916/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 917/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 918/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 919/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 920/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 921/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 922/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 923/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 924/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 925/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 926/1000\n",
      "1036/1036 [==============================] - 0s 59us/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 927/1000\n",
      "1036/1036 [==============================] - 0s 60us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 928/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 929/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 930/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 931/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 932/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 933/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 934/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 935/1000\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 936/1000\n",
      "1036/1036 [==============================] - 0s 49us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 937/1000\n",
      "1036/1036 [==============================] - 0s 50us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 938/1000\n",
      "1036/1036 [==============================] - 0s 53us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 939/1000\n",
      "1036/1036 [==============================] - 0s 51us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 940/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 941/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 942/1000\n",
      "1036/1036 [==============================] - ETA: 0s - loss: 0.058 - 0s 63us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 943/1000\n",
      "1036/1036 [==============================] - 0s 61us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 944/1000\n",
      "1036/1036 [==============================] - 0s 78us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 945/1000\n",
      "1036/1036 [==============================] - 0s 76us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 946/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 947/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 948/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 949/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 950/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 951/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 952/1000\n",
      "1036/1036 [==============================] - 0s 74us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 953/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 954/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 955/1000\n",
      "1036/1036 [==============================] - 0s 63us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 956/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 957/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 958/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 959/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 960/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 961/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 962/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 963/1000\n",
      "1036/1036 [==============================] - 0s 68us/step - loss: 0.0611 - val_loss: 0.0609\n",
      "Epoch 964/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 965/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0612 - val_loss: 0.0610\n",
      "Epoch 966/1000\n",
      "1036/1036 [==============================] - 0s 67us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 967/1000\n",
      "1036/1036 [==============================] - 0s 71us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 968/1000\n",
      "1036/1036 [==============================] - 0s 77us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 969/1000\n",
      "1036/1036 [==============================] - 0s 70us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 970/1000\n",
      "1036/1036 [==============================] - 0s 65us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 971/1000\n",
      "1036/1036 [==============================] - 0s 64us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 972/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 973/1000\n",
      "1036/1036 [==============================] - 0s 66us/step - loss: 0.0614 - val_loss: 0.0603\n",
      "Epoch 974/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 975/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 976/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 977/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 978/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 979/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0607\n",
      "Epoch 980/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 981/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 982/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 983/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 984/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 985/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 986/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 987/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0607\n",
      "Epoch 988/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0612 - val_loss: 0.0609\n",
      "Epoch 989/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 990/1000\n",
      "1036/1036 [==============================] - 0s 54us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 991/1000\n",
      "1036/1036 [==============================] - 0s 58us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 992/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0605\n",
      "Epoch 993/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 994/1000\n",
      "1036/1036 [==============================] - 0s 56us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 995/1000\n",
      "1036/1036 [==============================] - 0s 69us/step - loss: 0.0612 - val_loss: 0.0606\n",
      "Epoch 996/1000\n",
      "1036/1036 [==============================] - 0s 57us/step - loss: 0.0611 - val_loss: 0.0604\n",
      "Epoch 997/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 998/1000\n",
      "1036/1036 [==============================] - 0s 62us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 999/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0604\n",
      "Epoch 1000/1000\n",
      "1036/1036 [==============================] - 0s 55us/step - loss: 0.0612 - val_loss: 0.0605\n",
      "\n",
      "Epoch 01000: saving model to ./log_weights/Ide_AE_weights.1000.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint=ModelCheckpoint('./log_weights/Ide_AE_weights.{epoch:04d}.hdf5',period=100,save_weights_only=True,verbose=1)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(Ide_AE.layers[1].get_weights()))\n",
    "\n",
    "Ide_AE_history = Ide_AE.fit(x_train, x_train,\\\n",
    "                            epochs=epochs_number,\\\n",
    "                            batch_size=batch_size_value,\\\n",
    "                            shuffle=True,\\\n",
    "                            validation_data=(x_validate,x_validate),\\\n",
    "                            callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = Ide_AE_history.history['loss']\n",
    "val_loss = Ide_AE_history.history['val_loss']\n",
    "\n",
    "epochs = range(epochs_number)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for one-to-one map layer 0.05991638607449002\n"
     ]
    }
   ],
   "source": [
    "p_data=Ide_AE.predict(x_test)\n",
    "numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)-x_test,2))/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "key_number=50\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_number=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features=F.top_k_keepWeights_1(Ide_AE.get_layer(index=1).get_weights()[0],key_number)\n",
    "\n",
    "selected_position_list=np.where(key_features>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Training accuracy 1.0\n",
      "Testing accuracy 1.0\n",
      "Testing accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "train_feature=C_train_x\n",
    "train_label=C_train_y\n",
    "test_feature=C_test_x\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 5)\n",
      "(288, 5)\n",
      "Training accuracy 1.0\n",
      "Training accuracy 1.0\n",
      "Testing accuracy 0.8923611111111112\n",
      "Testing accuracy 0.8923611111111112\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1152, 5)\n",
      "(288, 5)\n",
      "0.03813062965835352\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(C_train_selected_x.shape)\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(C_test_selected_x.shape)\n",
    "\n",
    "\n",
    "train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
