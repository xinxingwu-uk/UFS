{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import random\n",
    "import scipy.sparse as sparse\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./Dataset/GLIOMA.mat\"\n",
    "Data = scipy.io.loadmat(data_path)\n",
    "\n",
    "data_arr=Data['X']\n",
    "label_arr=Data['Y'][:, 0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=MinMaxScaler(feature_range=(0,1)).fit_transform(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of C_train_x: (40, 4434)\n",
      "Shape of C_train_y: (40,)\n",
      "Shape of C_test_x: (10, 4434)\n",
      "Shape of C_test_y: (10,)\n",
      "Shape of x_train: (36, 4434)\n",
      "Shape of x_test: (10, 4434)\n",
      "Shape of x_validate: (4, 4434)\n",
      "Shape of y_train: (36,)\n",
      "Shape of y_test: (10,)\n",
      "Shape of y_validate: (4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ny_train_onehot = to_categorical(y_train_)\\ny_validate_onehot = to_categorical(y_validate_)\\ny_test_onehot = to_categorical(C_test_y)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(Data,label_arr,test_size=0.2,random_state=seed)\n",
    "x_train,x_validate,y_train,y_validate= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=seed)\n",
    "\n",
    "print('Shape of C_train_x: ' + str(C_train_x.shape)) \n",
    "print('Shape of C_train_y: ' + str(C_train_y.shape)) \n",
    "print('Shape of C_test_x: ' + str(C_test_x.shape)) \n",
    "print('Shape of C_test_y: ' + str(C_test_y.shape)) \n",
    "\n",
    "x_test=C_test_x\n",
    "y_test=C_test_y\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape)) \n",
    "print('Shape of x_validate: ' + str(x_validate.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "print('Shape of y_validate: ' + str(y_validate.shape))\n",
    "\n",
    "'''\n",
    "y_train_onehot = to_categorical(y_train_)\n",
    "y_validate_onehot = to_categorical(y_validate_)\n",
    "y_test_onehot = to_categorical(C_test_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "class Feature_Select_Layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, l1_lambda, **kwargs):\n",
    "        super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.l1_lambda=l1_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0., maxval=1.),\n",
    "                                      trainable=True,\n",
    "                                      regularizer=regularizers.l1(self.l1_lambda),\n",
    "                                      constraint=constraints.NonNeg())\n",
    "        super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, selection=False,k=36):\n",
    "        kernel=self.kernel        \n",
    "        if selection:\n",
    "            kernel_=K.transpose(kernel)\n",
    "            print(kernel_.shape)\n",
    "            kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "            kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "        return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                         p_encoding_dim=50,\\\n",
    "                         p_learning_rate= 1E-3,\\\n",
    "                         p_l1_lambda=0.1):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             l1_lambda=p_l1_lambda,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,latent_encoder_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "batch_size_value=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1.1 Identity Autoencoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Autoencoder Structure-------------------------------------\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye (None, 4434)              0         \n",
      "_________________________________________________________________\n",
      "feature_selection (Feature_S (None, 4434)              4434      \n",
      "_________________________________________________________________\n",
      "autoencoder_hidden_layer (De (None, 64)                283840    \n",
      "_________________________________________________________________\n",
      "autoencoder_output (Dense)   (None, 4434)              288210    \n",
      "=================================================================\n",
      "Total params: 576,484\n",
      "Trainable params: 576,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ide_AE,\\\n",
    "latent_encoder_score_Ide_AE=Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                                 p_encoding_dim=64,\\\n",
    "                                                 p_learning_rate= 1E-2,\\\n",
    "                                                 p_l1_lambda=l1_lambda)\n",
    "\n",
    "file_name=\"./log/AgnoSS.png\"\n",
    "plot_model(Ide_AE, to_file=file_name,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 36 samples, validate on 4 samples\n",
      "Epoch 1/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 2160.6787 - val_loss: 2072.8782\n",
      "Epoch 2/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 2046.0248 - val_loss: 1951.4393\n",
      "Epoch 3/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 1924.4965 - val_loss: 1830.4961\n",
      "Epoch 4/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 1804.0765 - val_loss: 1712.2617\n",
      "Epoch 5/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 1686.5271 - val_loss: 1597.3612\n",
      "Epoch 6/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 1572.3565 - val_loss: 1485.7037\n",
      "Epoch 7/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 1461.5869 - val_loss: 1378.2677\n",
      "Epoch 8/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 1354.9769 - val_loss: 1274.3707\n",
      "Epoch 9/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 1251.8379 - val_loss: 1174.0327\n",
      "Epoch 10/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 1152.3967 - val_loss: 1077.8671\n",
      "Epoch 11/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 1057.2310 - val_loss: 986.2114\n",
      "Epoch 12/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 966.5068 - val_loss: 898.7337\n",
      "Epoch 13/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 879.9087 - val_loss: 815.0391\n",
      "Epoch 14/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 797.0354 - val_loss: 735.2258\n",
      "Epoch 15/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 718.2301 - val_loss: 659.9052\n",
      "Epoch 16/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 643.8717 - val_loss: 588.9440\n",
      "Epoch 17/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 573.7688 - val_loss: 521.8625\n",
      "Epoch 18/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 507.5928 - val_loss: 459.0739\n",
      "Epoch 19/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 445.8812 - val_loss: 400.7464\n",
      "Epoch 20/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 388.3857 - val_loss: 346.1564\n",
      "Epoch 21/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 334.5782 - val_loss: 295.2994\n",
      "Epoch 22/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 284.6548 - val_loss: 248.5157\n",
      "Epoch 23/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 238.7733 - val_loss: 205.7579\n",
      "Epoch 24/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 196.8554 - val_loss: 166.8000\n",
      "Epoch 25/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 158.7929 - val_loss: 132.0237\n",
      "Epoch 26/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 125.0143 - val_loss: 101.6985\n",
      "Epoch 27/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 95.5489 - val_loss: 75.1841\n",
      "Epoch 28/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 69.9911 - val_loss: 52.9112\n",
      "Epoch 29/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 48.5400 - val_loss: 34.4104\n",
      "Epoch 30/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 31.0135 - val_loss: 20.1462\n",
      "Epoch 31/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 17.5623 - val_loss: 9.7215\n",
      "Epoch 32/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 8.0278 - val_loss: 3.1985\n",
      "Epoch 33/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 2.2940 - val_loss: 0.3163\n",
      "Epoch 34/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.1665 - val_loss: 0.0757\n",
      "Epoch 35/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.0599 - val_loss: 0.0602\n",
      "Epoch 36/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0713 - val_loss: 0.0608\n",
      "Epoch 37/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0766 - val_loss: 0.0600\n",
      "Epoch 38/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0705 - val_loss: 0.0619\n",
      "Epoch 39/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0627 - val_loss: 0.0701\n",
      "Epoch 40/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0621 - val_loss: 0.0812\n",
      "Epoch 41/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0640 - val_loss: 0.0845\n",
      "Epoch 42/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0649 - val_loss: 0.0891\n",
      "Epoch 43/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0692 - val_loss: 0.0666\n",
      "Epoch 44/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0610 - val_loss: 0.0638\n",
      "Epoch 45/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0614 - val_loss: 0.0652\n",
      "Epoch 46/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0622 - val_loss: 0.0642\n",
      "Epoch 47/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0662\n",
      "Epoch 48/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0611 - val_loss: 0.0720\n",
      "Epoch 49/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0625 - val_loss: 0.0688\n",
      "Epoch 50/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0606 - val_loss: 0.0700\n",
      "Epoch 51/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0612 - val_loss: 0.0703\n",
      "Epoch 52/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0610 - val_loss: 0.0786\n",
      "Epoch 53/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0651 - val_loss: 0.0702\n",
      "Epoch 54/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0629 - val_loss: 0.0690\n",
      "Epoch 55/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0607 - val_loss: 0.0699\n",
      "Epoch 56/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0611 - val_loss: 0.0704\n",
      "Epoch 57/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0610 - val_loss: 0.0712\n",
      "Epoch 58/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0611 - val_loss: 0.0710\n",
      "Epoch 59/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0609 - val_loss: 0.0713\n",
      "Epoch 60/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0616 - val_loss: 0.0678\n",
      "Epoch 61/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0607 - val_loss: 0.0687\n",
      "Epoch 62/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0670\n",
      "Epoch 63/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0611 - val_loss: 0.0680\n",
      "Epoch 64/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0619 - val_loss: 0.0693\n",
      "Epoch 65/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0625 - val_loss: 0.0710\n",
      "Epoch 66/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0616 - val_loss: 0.0735\n",
      "Epoch 67/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0621 - val_loss: 0.0713\n",
      "Epoch 68/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0611 - val_loss: 0.0732\n",
      "Epoch 69/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0621 - val_loss: 0.0719\n",
      "Epoch 70/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0627 - val_loss: 0.0651\n",
      "Epoch 71/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0615 - val_loss: 0.0649\n",
      "Epoch 72/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0615 - val_loss: 0.0665\n",
      "Epoch 73/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0610 - val_loss: 0.0698\n",
      "Epoch 74/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0618 - val_loss: 0.0729\n",
      "Epoch 75/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0621 - val_loss: 0.0963\n",
      "Epoch 76/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0719 - val_loss: 0.0781\n",
      "Epoch 77/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0635 - val_loss: 0.0703\n",
      "Epoch 78/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0630 - val_loss: 0.0642\n",
      "Epoch 79/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0619 - val_loss: 0.0649\n",
      "Epoch 80/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0698\n",
      "Epoch 81/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0607 - val_loss: 0.0751\n",
      "Epoch 82/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0628 - val_loss: 0.0810\n",
      "Epoch 83/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0649 - val_loss: 0.0809\n",
      "Epoch 84/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0649 - val_loss: 0.0682\n",
      "Epoch 85/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0644\n",
      "Epoch 86/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0629 - val_loss: 0.0636\n",
      "Epoch 87/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0668\n",
      "Epoch 88/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0611 - val_loss: 0.0700\n",
      "Epoch 89/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0617 - val_loss: 0.0789\n",
      "Epoch 90/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0647 - val_loss: 0.0737\n",
      "Epoch 91/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0616 - val_loss: 0.0749\n",
      "Epoch 92/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0624 - val_loss: 0.0707\n",
      "Epoch 93/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0690\n",
      "Epoch 94/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0609 - val_loss: 0.0718\n",
      "Epoch 95/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0621 - val_loss: 0.0696\n",
      "Epoch 96/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0623 - val_loss: 0.0659\n",
      "Epoch 97/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0612 - val_loss: 0.0688\n",
      "Epoch 98/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0629 - val_loss: 0.0641\n",
      "Epoch 99/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0663\n",
      "Epoch 100/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0620 - val_loss: 0.0700\n",
      "\n",
      "Epoch 00100: saving model to ./log_weights/Ide_AE_weights.0100.hdf5\n",
      "Epoch 101/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0631 - val_loss: 0.0687\n",
      "Epoch 102/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0675\n",
      "Epoch 103/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0749\n",
      "Epoch 104/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0637 - val_loss: 0.0710\n",
      "Epoch 105/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0611 - val_loss: 0.0733\n",
      "Epoch 106/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0628 - val_loss: 0.0703\n",
      "Epoch 107/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0613 - val_loss: 0.0734\n",
      "Epoch 108/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0632 - val_loss: 0.0690\n",
      "Epoch 109/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0610 - val_loss: 0.0709\n",
      "Epoch 110/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0617 - val_loss: 0.0717\n",
      "Epoch 111/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0625 - val_loss: 0.0731\n",
      "Epoch 112/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0723\n",
      "Epoch 113/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0626 - val_loss: 0.0684\n",
      "Epoch 114/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0618 - val_loss: 0.0655\n",
      "Epoch 115/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0665\n",
      "Epoch 116/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0659\n",
      "Epoch 117/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.0787\n",
      "Epoch 118/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0664 - val_loss: 0.0686\n",
      "Epoch 119/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0610 - val_loss: 0.0716\n",
      "Epoch 120/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0609 - val_loss: 0.0759\n",
      "Epoch 121/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.0800\n",
      "Epoch 122/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0642 - val_loss: 0.0810\n",
      "Epoch 123/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0662 - val_loss: 0.0800\n",
      "Epoch 124/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0648 - val_loss: 0.0681\n",
      "Epoch 125/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0611 - val_loss: 0.0693\n",
      "Epoch 126/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0642 - val_loss: 0.0609\n",
      "Epoch 127/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0625 - val_loss: 0.0613\n",
      "Epoch 128/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0621 - val_loss: 0.0640\n",
      "Epoch 129/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0611 - val_loss: 0.0691\n",
      "Epoch 130/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0770\n",
      "Epoch 131/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0636 - val_loss: 0.0752\n",
      "Epoch 132/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0626 - val_loss: 0.0724\n",
      "Epoch 133/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0614 - val_loss: 0.0725\n",
      "Epoch 134/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0619 - val_loss: 0.0788\n",
      "Epoch 135/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0661 - val_loss: 0.0654\n",
      "Epoch 136/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0658\n",
      "Epoch 137/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0685\n",
      "Epoch 138/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0610 - val_loss: 0.0730\n",
      "Epoch 139/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0620 - val_loss: 0.0799\n",
      "Epoch 140/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0649 - val_loss: 0.0748\n",
      "Epoch 141/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0623 - val_loss: 0.0750\n",
      "Epoch 142/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0629 - val_loss: 0.0720\n",
      "Epoch 143/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0623 - val_loss: 0.0698\n",
      "Epoch 144/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0617 - val_loss: 0.0695\n",
      "Epoch 145/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0622 - val_loss: 0.0757\n",
      "Epoch 146/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0661 - val_loss: 0.0684\n",
      "Epoch 147/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0644 - val_loss: 0.0635\n",
      "Epoch 148/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0670\n",
      "Epoch 149/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0740\n",
      "Epoch 150/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0625 - val_loss: 0.0872\n",
      "Epoch 151/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0679 - val_loss: 0.0720\n",
      "Epoch 152/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0632 - val_loss: 0.0656\n",
      "Epoch 153/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0698\n",
      "Epoch 154/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0641 - val_loss: 0.0658\n",
      "Epoch 155/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0625 - val_loss: 0.0676\n",
      "Epoch 156/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0626 - val_loss: 0.0668\n",
      "Epoch 157/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0714\n",
      "Epoch 158/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0627 - val_loss: 0.0703\n",
      "Epoch 159/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0610 - val_loss: 0.0780\n",
      "Epoch 160/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0639 - val_loss: 0.0817\n",
      "Epoch 161/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0666 - val_loss: 0.0680\n",
      "Epoch 162/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0607 - val_loss: 0.0696\n",
      "Epoch 163/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0619 - val_loss: 0.0674\n",
      "Epoch 164/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0823\n",
      "Epoch 165/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0683 - val_loss: 0.0658\n",
      "Epoch 166/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0625 - val_loss: 0.0679\n",
      "Epoch 167/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0637 - val_loss: 0.0659\n",
      "Epoch 168/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0629 - val_loss: 0.0676\n",
      "Epoch 169/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0629 - val_loss: 0.0686\n",
      "Epoch 170/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0619 - val_loss: 0.0730\n",
      "Epoch 171/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0637 - val_loss: 0.0750\n",
      "Epoch 172/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0626 - val_loss: 0.0774\n",
      "Epoch 173/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0632 - val_loss: 0.0797\n",
      "Epoch 174/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0639 - val_loss: 0.0740\n",
      "Epoch 175/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0623 - val_loss: 0.0727\n",
      "Epoch 176/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0651 - val_loss: 0.0636\n",
      "Epoch 177/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0630 - val_loss: 0.0630\n",
      "Epoch 178/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0673\n",
      "Epoch 179/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0622 - val_loss: 0.0683\n",
      "Epoch 180/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0617 - val_loss: 0.0726\n",
      "Epoch 181/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0622 - val_loss: 0.0748\n",
      "Epoch 182/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0628 - val_loss: 0.0840\n",
      "Epoch 183/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0668 - val_loss: 0.0700\n",
      "Epoch 184/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.0679\n",
      "Epoch 185/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0623 - val_loss: 0.0664\n",
      "Epoch 186/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0629 - val_loss: 0.0649\n",
      "Epoch 187/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0688\n",
      "Epoch 188/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0745\n",
      "Epoch 189/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0629 - val_loss: 0.0742\n",
      "Epoch 190/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0618 - val_loss: 0.0868\n",
      "Epoch 191/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0674 - val_loss: 0.0704\n",
      "Epoch 192/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0624 - val_loss: 0.0708\n",
      "Epoch 193/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0646 - val_loss: 0.0641\n",
      "Epoch 194/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0620 - val_loss: 0.0650\n",
      "Epoch 195/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0615 - val_loss: 0.0673\n",
      "Epoch 196/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0760\n",
      "Epoch 197/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0641 - val_loss: 0.0775\n",
      "Epoch 198/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0647 - val_loss: 0.0707\n",
      "Epoch 199/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0624 - val_loss: 0.0672\n",
      "Epoch 200/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0607 - val_loss: 0.0820\n",
      "\n",
      "Epoch 00200: saving model to ./log_weights/Ide_AE_weights.0200.hdf5\n",
      "Epoch 201/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0680 - val_loss: 0.0699\n",
      "Epoch 202/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0639 - val_loss: 0.0645\n",
      "Epoch 203/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0616 - val_loss: 0.0727\n",
      "Epoch 204/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0645 - val_loss: 0.0678\n",
      "Epoch 205/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0768\n",
      "Epoch 206/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0645 - val_loss: 0.0690\n",
      "Epoch 207/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0692\n",
      "Epoch 208/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0610 - val_loss: 0.0855\n",
      "Epoch 209/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0694 - val_loss: 0.0682\n",
      "Epoch 210/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.0714\n",
      "Epoch 211/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0628 - val_loss: 0.0676\n",
      "Epoch 212/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0810\n",
      "Epoch 213/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0668 - val_loss: 0.0732\n",
      "Epoch 214/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0642 - val_loss: 0.0709\n",
      "Epoch 215/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0647 - val_loss: 0.0673\n",
      "Epoch 216/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0648 - val_loss: 0.0632\n",
      "Epoch 217/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0622 - val_loss: 0.0660\n",
      "Epoch 218/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0701\n",
      "Epoch 219/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0616 - val_loss: 0.0816\n",
      "Epoch 220/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0655 - val_loss: 0.0997\n",
      "Epoch 221/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0730 - val_loss: 0.0706\n",
      "Epoch 222/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0615 - val_loss: 0.0667\n",
      "Epoch 223/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0665\n",
      "Epoch 224/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0607 - val_loss: 0.0700\n",
      "Epoch 225/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0618 - val_loss: 0.0731\n",
      "Epoch 226/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0621 - val_loss: 0.0747\n",
      "Epoch 227/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0626 - val_loss: 0.0729\n",
      "Epoch 228/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0618 - val_loss: 0.0702\n",
      "Epoch 229/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0612 - val_loss: 0.0682\n",
      "Epoch 230/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0614 - val_loss: 0.0679\n",
      "Epoch 231/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0613 - val_loss: 0.0695\n",
      "Epoch 232/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0616 - val_loss: 0.0730\n",
      "Epoch 233/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0628 - val_loss: 0.0719\n",
      "Epoch 234/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0739\n",
      "Epoch 235/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0622 - val_loss: 0.0738\n",
      "Epoch 236/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0623 - val_loss: 0.0707\n",
      "Epoch 237/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0715\n",
      "Epoch 238/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0625 - val_loss: 0.0673\n",
      "Epoch 239/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0610 - val_loss: 0.0683\n",
      "Epoch 240/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0987\n",
      "Epoch 241/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0753 - val_loss: 0.0654\n",
      "Epoch 242/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0680\n",
      "Epoch 243/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0630 - val_loss: 0.0685\n",
      "Epoch 244/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0625 - val_loss: 0.0686\n",
      "Epoch 245/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0707\n",
      "Epoch 246/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0808\n",
      "Epoch 247/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0661 - val_loss: 0.0728\n",
      "Epoch 248/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0725\n",
      "Epoch 249/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0617 - val_loss: 0.0739\n",
      "Epoch 250/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0628 - val_loss: 0.0697\n",
      "Epoch 251/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0683\n",
      "Epoch 252/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0695\n",
      "Epoch 253/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0617 - val_loss: 0.0699\n",
      "Epoch 254/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0614 - val_loss: 0.0758\n",
      "Epoch 255/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0641 - val_loss: 0.0699\n",
      "Epoch 256/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0610 - val_loss: 0.0748\n",
      "Epoch 257/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0636 - val_loss: 0.0697\n",
      "Epoch 258/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0620 - val_loss: 0.0702\n",
      "Epoch 259/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0631 - val_loss: 0.0687\n",
      "Epoch 260/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0633 - val_loss: 0.0652\n",
      "Epoch 261/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0765\n",
      "Epoch 262/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0663 - val_loss: 0.0679\n",
      "Epoch 263/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0611 - val_loss: 0.0768\n",
      "Epoch 264/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0647 - val_loss: 0.0707\n",
      "Epoch 265/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0727\n",
      "Epoch 266/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0623 - val_loss: 0.0744\n",
      "Epoch 267/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0645 - val_loss: 0.0659\n",
      "Epoch 268/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0708\n",
      "Epoch 269/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0640 - val_loss: 0.0646\n",
      "Epoch 270/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0660\n",
      "Epoch 271/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0682\n",
      "Epoch 272/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0616 - val_loss: 0.0834\n",
      "Epoch 273/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0678 - val_loss: 0.0711\n",
      "Epoch 274/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0731\n",
      "Epoch 275/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0614 - val_loss: 0.0810\n",
      "Epoch 276/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0655 - val_loss: 0.0714\n",
      "Epoch 277/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0610 - val_loss: 0.0859\n",
      "Epoch 278/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0696 - val_loss: 0.0658\n",
      "Epoch 279/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0615 - val_loss: 0.0666\n",
      "Epoch 280/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0607 - val_loss: 0.0716\n",
      "Epoch 281/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0620 - val_loss: 0.0768\n",
      "Epoch 282/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0639 - val_loss: 0.0759\n",
      "Epoch 283/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0637 - val_loss: 0.0753\n",
      "Epoch 284/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0628 - val_loss: 0.0737\n",
      "Epoch 285/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0625 - val_loss: 0.0688\n",
      "Epoch 286/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0706\n",
      "Epoch 287/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0630 - val_loss: 0.0759\n",
      "Epoch 288/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0662 - val_loss: 0.0640\n",
      "Epoch 289/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0621 - val_loss: 0.0633\n",
      "Epoch 290/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0618 - val_loss: 0.0675\n",
      "Epoch 291/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0630 - val_loss: 0.0673\n",
      "Epoch 292/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0717\n",
      "Epoch 293/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0621 - val_loss: 0.0786\n",
      "Epoch 294/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0649 - val_loss: 0.0725\n",
      "Epoch 295/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0623 - val_loss: 0.0772\n",
      "Epoch 296/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0647 - val_loss: 0.0695\n",
      "Epoch 297/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0693\n",
      "Epoch 298/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0623 - val_loss: 0.0703\n",
      "Epoch 299/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0611 - val_loss: 0.0781\n",
      "Epoch 300/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0648 - val_loss: 0.0774\n",
      "\n",
      "Epoch 00300: saving model to ./log_weights/Ide_AE_weights.0300.hdf5\n",
      "Epoch 301/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0641 - val_loss: 0.0710\n",
      "Epoch 302/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0614 - val_loss: 0.0705\n",
      "Epoch 303/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.0697\n",
      "Epoch 304/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0813\n",
      "Epoch 305/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0661 - val_loss: 0.0920\n",
      "Epoch 306/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0713 - val_loss: 0.0676\n",
      "Epoch 307/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0613 - val_loss: 0.0671\n",
      "Epoch 308/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0689\n",
      "Epoch 309/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0795\n",
      "Epoch 310/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0654 - val_loss: 0.0720\n",
      "Epoch 311/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0620 - val_loss: 0.0753\n",
      "Epoch 312/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0638 - val_loss: 0.0681\n",
      "Epoch 313/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0611 - val_loss: 0.0675\n",
      "Epoch 314/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.0684\n",
      "Epoch 315/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0609 - val_loss: 0.0747\n",
      "Epoch 316/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0642 - val_loss: 0.0783\n",
      "Epoch 317/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0652 - val_loss: 0.0758\n",
      "Epoch 318/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0627 - val_loss: 0.0840\n",
      "Epoch 319/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0662 - val_loss: 0.0715\n",
      "Epoch 320/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0634 - val_loss: 0.0635\n",
      "Epoch 321/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0613 - val_loss: 0.0643\n",
      "Epoch 322/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0613 - val_loss: 0.0763\n",
      "Epoch 323/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0658 - val_loss: 0.0784\n",
      "Epoch 324/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0669 - val_loss: 0.0657\n",
      "Epoch 325/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0609 - val_loss: 0.0718\n",
      "Epoch 326/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0631 - val_loss: 0.0737\n",
      "Epoch 327/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0650 - val_loss: 0.0700\n",
      "Epoch 328/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0714\n",
      "Epoch 329/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0626 - val_loss: 0.0736\n",
      "Epoch 330/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0632 - val_loss: 0.0691\n",
      "Epoch 331/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0609 - val_loss: 0.0832\n",
      "Epoch 332/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0677 - val_loss: 0.0663\n",
      "Epoch 333/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0619 - val_loss: 0.0657\n",
      "Epoch 334/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0611 - val_loss: 0.0720\n",
      "Epoch 335/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0636 - val_loss: 0.0743\n",
      "Epoch 336/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0640 - val_loss: 0.0742\n",
      "Epoch 337/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0640 - val_loss: 0.0713\n",
      "Epoch 338/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0624 - val_loss: 0.0699\n",
      "Epoch 339/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0618 - val_loss: 0.0830\n",
      "Epoch 340/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0675 - val_loss: 0.0766\n",
      "Epoch 341/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0662 - val_loss: 0.0649\n",
      "Epoch 342/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0623 - val_loss: 0.0639\n",
      "Epoch 343/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0713\n",
      "Epoch 344/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0635 - val_loss: 0.0710\n",
      "Epoch 345/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.0862\n",
      "Epoch 346/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0695 - val_loss: 0.0769\n",
      "Epoch 347/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0661 - val_loss: 0.0676\n",
      "Epoch 348/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0650 - val_loss: 0.0637\n",
      "Epoch 349/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0615 - val_loss: 0.0678\n",
      "Epoch 350/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0610 - val_loss: 0.0731\n",
      "Epoch 351/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0636 - val_loss: 0.0756\n",
      "Epoch 352/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0629 - val_loss: 0.0787\n",
      "Epoch 353/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0646 - val_loss: 0.0792\n",
      "Epoch 354/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0646 - val_loss: 0.0758\n",
      "Epoch 355/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0629 - val_loss: 0.0784\n",
      "Epoch 356/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0668 - val_loss: 0.0655\n",
      "Epoch 357/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0631 - val_loss: 0.0626\n",
      "Epoch 358/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0626 - val_loss: 0.0641\n",
      "Epoch 359/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0621 - val_loss: 0.0727\n",
      "Epoch 360/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0645 - val_loss: 0.0760\n",
      "Epoch 361/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0649 - val_loss: 0.0755\n",
      "Epoch 362/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0650 - val_loss: 0.0686\n",
      "Epoch 363/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0621 - val_loss: 0.0687\n",
      "Epoch 364/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0786\n",
      "Epoch 365/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0656 - val_loss: 0.0743\n",
      "Epoch 366/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0644 - val_loss: 0.0684\n",
      "Epoch 367/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0632 - val_loss: 0.0647\n",
      "Epoch 368/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0615 - val_loss: 0.0700\n",
      "Epoch 369/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0636 - val_loss: 0.0652\n",
      "Epoch 370/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0670\n",
      "Epoch 371/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0617 - val_loss: 0.0701\n",
      "Epoch 372/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.0733\n",
      "Epoch 373/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0645 - val_loss: 0.0751\n",
      "Epoch 374/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0653 - val_loss: 0.0813\n",
      "Epoch 375/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0654 - val_loss: 0.0860\n",
      "Epoch 376/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0693 - val_loss: 0.0719\n",
      "Epoch 377/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0658 - val_loss: 0.0625\n",
      "Epoch 378/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0627 - val_loss: 0.0645\n",
      "Epoch 379/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0619 - val_loss: 0.0685\n",
      "Epoch 380/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0784\n",
      "Epoch 381/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0662 - val_loss: 0.0746\n",
      "Epoch 382/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0620 - val_loss: 0.0934\n",
      "Epoch 383/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0710 - val_loss: 0.0749\n",
      "Epoch 384/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0635 - val_loss: 0.0693\n",
      "Epoch 385/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0617 - val_loss: 0.0883\n",
      "Epoch 386/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0706 - val_loss: 0.0684\n",
      "Epoch 387/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0634 - val_loss: 0.0659\n",
      "Epoch 388/1000\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.0633 - val_loss: 0.0640\n",
      "Epoch 389/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0614 - val_loss: 0.0687\n",
      "Epoch 390/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0621 - val_loss: 0.0854\n",
      "Epoch 391/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0700 - val_loss: 0.0746\n",
      "Epoch 392/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0632 - val_loss: 0.0742\n",
      "Epoch 393/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0626 - val_loss: 0.0829\n",
      "Epoch 394/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0665 - val_loss: 0.0776\n",
      "Epoch 395/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0679 - val_loss: 0.0633\n",
      "Epoch 396/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0620 - val_loss: 0.0637\n",
      "Epoch 397/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0617 - val_loss: 0.0682\n",
      "Epoch 398/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0630 - val_loss: 0.0681\n",
      "Epoch 399/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0616 - val_loss: 0.0705\n",
      "Epoch 400/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0623 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00400: saving model to ./log_weights/Ide_AE_weights.0400.hdf5\n",
      "Epoch 401/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.0639 - val_loss: 0.0795\n",
      "Epoch 402/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0652 - val_loss: 0.0706\n",
      "Epoch 403/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0613 - val_loss: 0.0701\n",
      "Epoch 404/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0622 - val_loss: 0.0696\n",
      "Epoch 405/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0636 - val_loss: 0.0691\n",
      "Epoch 406/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0615 - val_loss: 0.0813\n",
      "Epoch 407/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0669 - val_loss: 0.0698\n",
      "Epoch 408/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0613 - val_loss: 0.0809\n",
      "Epoch 409/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0670 - val_loss: 0.0666\n",
      "Epoch 410/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0617 - val_loss: 0.0670\n",
      "Epoch 411/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0613 - val_loss: 0.0763\n",
      "Epoch 412/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0642 - val_loss: 0.0829\n",
      "Epoch 413/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0663 - val_loss: 0.0742\n",
      "Epoch 414/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0640 - val_loss: 0.0688\n",
      "Epoch 415/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0608 - val_loss: 0.0772\n",
      "Epoch 416/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.0651 - val_loss: 0.0678\n",
      "Epoch 417/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0610 - val_loss: 0.0724\n",
      "Epoch 418/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.1008\n",
      "Epoch 419/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0760 - val_loss: 0.0665\n",
      "Epoch 420/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0611 - val_loss: 0.0681\n",
      "Epoch 421/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0621 - val_loss: 0.0680\n",
      "Epoch 422/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0605 - val_loss: 0.0727\n",
      "Epoch 423/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0627 - val_loss: 0.0852\n",
      "Epoch 424/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0679 - val_loss: 0.0766\n",
      "Epoch 425/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0645 - val_loss: 0.0704\n",
      "Epoch 426/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0686\n",
      "Epoch 427/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0609 - val_loss: 0.0727\n",
      "Epoch 428/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 429/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0615 - val_loss: 0.0666\n",
      "Epoch 430/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0613 - val_loss: 0.0710\n",
      "Epoch 431/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0628 - val_loss: 0.0802\n",
      "Epoch 432/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0664 - val_loss: 0.0711\n",
      "Epoch 433/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0626 - val_loss: 0.0709\n",
      "Epoch 434/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0623 - val_loss: 0.0723\n",
      "Epoch 435/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0623 - val_loss: 0.0770\n",
      "Epoch 436/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0635 - val_loss: 0.0903\n",
      "Epoch 437/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0717 - val_loss: 0.0647\n",
      "Epoch 438/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0611 - val_loss: 0.0657\n",
      "Epoch 439/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0612 - val_loss: 0.0679\n",
      "Epoch 440/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0633 - val_loss: 0.0722\n",
      "Epoch 441/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0635 - val_loss: 0.0726\n",
      "Epoch 442/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0627 - val_loss: 0.0731\n",
      "Epoch 443/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0640 - val_loss: 0.0730\n",
      "Epoch 444/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0637 - val_loss: 0.0728\n",
      "Epoch 445/1000\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 0.0642 - val_loss: 0.0726\n",
      "Epoch 446/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.0741\n",
      "Epoch 447/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0762\n",
      "Epoch 448/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0637 - val_loss: 0.0790\n",
      "Epoch 449/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0653 - val_loss: 0.0716\n",
      "Epoch 450/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0615 - val_loss: 0.0724\n",
      "Epoch 451/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0728\n",
      "Epoch 452/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0622 - val_loss: 0.0727\n",
      "Epoch 453/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0631 - val_loss: 0.0716\n",
      "Epoch 454/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0763\n",
      "Epoch 455/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0637 - val_loss: 0.0827\n",
      "Epoch 456/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0684 - val_loss: 0.0669\n",
      "Epoch 457/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0608 - val_loss: 0.0690\n",
      "Epoch 458/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0617 - val_loss: 0.0700\n",
      "Epoch 459/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0611 - val_loss: 0.0954\n",
      "Epoch 460/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0729 - val_loss: 0.0690\n",
      "Epoch 461/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0681\n",
      "Epoch 462/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0611 - val_loss: 0.0744\n",
      "Epoch 463/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0678 - val_loss: 0.0667\n",
      "Epoch 464/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0608 - val_loss: 0.0767\n",
      "Epoch 465/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0655 - val_loss: 0.0711\n",
      "Epoch 466/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0621 - val_loss: 0.0933\n",
      "Epoch 467/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0718 - val_loss: 0.0761\n",
      "Epoch 468/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0671 - val_loss: 0.0657\n",
      "Epoch 469/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0618 - val_loss: 0.0669\n",
      "Epoch 470/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0618 - val_loss: 0.0701\n",
      "Epoch 471/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0619 - val_loss: 0.0750\n",
      "Epoch 472/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0646 - val_loss: 0.0774\n",
      "Epoch 473/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0643 - val_loss: 0.0799\n",
      "Epoch 474/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0653 - val_loss: 0.0688\n",
      "Epoch 475/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0612 - val_loss: 0.0668\n",
      "Epoch 476/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0609 - val_loss: 0.0684\n",
      "Epoch 477/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0621 - val_loss: 0.0735\n",
      "Epoch 478/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0655 - val_loss: 0.0671\n",
      "Epoch 479/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0611 - val_loss: 0.0700\n",
      "Epoch 480/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0611 - val_loss: 0.0742\n",
      "Epoch 481/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0638 - val_loss: 0.0751\n",
      "Epoch 482/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0627 - val_loss: 0.0792\n",
      "Epoch 483/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0655 - val_loss: 0.0779\n",
      "Epoch 484/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0647 - val_loss: 0.0709\n",
      "Epoch 485/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0617 - val_loss: 0.0708\n",
      "Epoch 486/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0630 - val_loss: 0.0660\n",
      "Epoch 487/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0618 - val_loss: 0.0642\n",
      "Epoch 488/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0615 - val_loss: 0.0653\n",
      "Epoch 489/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0618 - val_loss: 0.0680\n",
      "Epoch 490/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0618 - val_loss: 0.0951\n",
      "Epoch 491/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0744 - val_loss: 0.0693\n",
      "Epoch 492/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0619 - val_loss: 0.0770\n",
      "Epoch 493/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0651 - val_loss: 0.0770\n",
      "Epoch 494/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0663 - val_loss: 0.0710\n",
      "Epoch 495/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0644 - val_loss: 0.0662\n",
      "Epoch 496/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0697\n",
      "Epoch 497/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0617 - val_loss: 0.0816\n",
      "Epoch 498/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0662 - val_loss: 0.0712\n",
      "Epoch 499/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0626 - val_loss: 0.0703\n",
      "Epoch 500/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0627 - val_loss: 0.0699\n",
      "\n",
      "Epoch 00500: saving model to ./log_weights/Ide_AE_weights.0500.hdf5\n",
      "Epoch 501/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0624 - val_loss: 0.0739\n",
      "Epoch 502/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0640 - val_loss: 0.0773\n",
      "Epoch 503/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0659 - val_loss: 0.0704\n",
      "Epoch 504/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0630 - val_loss: 0.0703\n",
      "Epoch 505/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0633 - val_loss: 0.0668\n",
      "Epoch 506/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0617 - val_loss: 0.0699\n",
      "Epoch 507/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0626 - val_loss: 0.0865\n",
      "Epoch 508/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0692 - val_loss: 0.0682\n",
      "Epoch 509/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0611 - val_loss: 0.0863\n",
      "Epoch 510/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0698 - val_loss: 0.0662\n",
      "Epoch 511/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0638 - val_loss: 0.0665\n",
      "Epoch 512/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0611 - val_loss: 0.0769\n",
      "Epoch 513/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0656 - val_loss: 0.0714\n",
      "Epoch 514/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0620 - val_loss: 0.0733\n",
      "Epoch 515/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0633 - val_loss: 0.0748\n",
      "Epoch 516/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0640 - val_loss: 0.0746\n",
      "Epoch 517/1000\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.0632 - val_loss: 0.0750\n",
      "Epoch 518/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0638 - val_loss: 0.0756\n",
      "Epoch 519/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0651 - val_loss: 0.0679\n",
      "Epoch 520/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0633 - val_loss: 0.0645\n",
      "Epoch 521/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0625 - val_loss: 0.0653\n",
      "Epoch 522/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0619 - val_loss: 0.0683\n",
      "Epoch 523/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.0733\n",
      "Epoch 524/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0633 - val_loss: 0.0879\n",
      "Epoch 525/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0695 - val_loss: 0.0811\n",
      "Epoch 526/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0656 - val_loss: 0.0712\n",
      "Epoch 527/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0622 - val_loss: 0.0840\n",
      "Epoch 528/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0693 - val_loss: 0.0655\n",
      "Epoch 529/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0641 - val_loss: 0.0615\n",
      "Epoch 530/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0628 - val_loss: 0.0651\n",
      "Epoch 531/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0631 - val_loss: 0.0719\n",
      "Epoch 532/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0643 - val_loss: 0.0685\n",
      "Epoch 533/1000\n",
      "36/36 [==============================] - 1s 32ms/step - loss: 0.0617 - val_loss: 0.0721\n",
      "Epoch 534/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0634 - val_loss: 0.0735\n",
      "Epoch 535/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0626 - val_loss: 0.0784\n",
      "Epoch 536/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0648 - val_loss: 0.0829\n",
      "Epoch 537/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0667 - val_loss: 0.0761\n",
      "Epoch 538/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0638 - val_loss: 0.0695\n",
      "Epoch 539/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.0657\n",
      "Epoch 540/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0609 - val_loss: 0.0688\n",
      "Epoch 541/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0612 - val_loss: 0.0947\n",
      "Epoch 542/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0724 - val_loss: 0.0780\n",
      "Epoch 543/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0644 - val_loss: 0.0792\n",
      "Epoch 544/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0658 - val_loss: 0.0681\n",
      "Epoch 545/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0633 - val_loss: 0.0697\n",
      "Epoch 546/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0637 - val_loss: 0.0666\n",
      "Epoch 547/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0620 - val_loss: 0.0682\n",
      "Epoch 548/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0626 - val_loss: 0.0712\n",
      "Epoch 549/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0632 - val_loss: 0.0702\n",
      "Epoch 550/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0732\n",
      "Epoch 551/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0623 - val_loss: 0.0848\n",
      "Epoch 552/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0680 - val_loss: 0.0726\n",
      "Epoch 553/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0633 - val_loss: 0.0709\n",
      "Epoch 554/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0617 - val_loss: 0.0852\n",
      "Epoch 555/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.0681 - val_loss: 0.0822\n",
      "Epoch 556/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.0691 - val_loss: 0.0634\n",
      "Epoch 557/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0615 - val_loss: 0.0642\n",
      "Epoch 558/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0723\n",
      "Epoch 559/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0668 - val_loss: 0.0680\n",
      "Epoch 560/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0620 - val_loss: 0.0715\n",
      "Epoch 561/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0638 - val_loss: 0.0767\n",
      "Epoch 562/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0645 - val_loss: 0.0865\n",
      "Epoch 563/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.0687 - val_loss: 0.0705\n",
      "Epoch 564/1000\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 0.0638 - val_loss: 0.0690\n",
      "Epoch 565/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.0610 - val_loss: 0.0717\n",
      "Epoch 566/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.0635 - val_loss: 0.0725\n",
      "Epoch 567/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0624 - val_loss: 0.0752\n",
      "Epoch 568/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0636 - val_loss: 0.0946\n",
      "Epoch 569/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0715 - val_loss: 0.0843\n",
      "Epoch 570/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0680 - val_loss: 0.0670\n",
      "Epoch 571/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0625 - val_loss: 0.0690\n",
      "Epoch 572/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0665 - val_loss: 0.0623\n",
      "Epoch 573/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0617 - val_loss: 0.0667\n",
      "Epoch 574/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0621 - val_loss: 0.0829\n",
      "Epoch 575/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0680 - val_loss: 0.0991\n",
      "Epoch 576/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0776 - val_loss: 0.0681\n",
      "Epoch 577/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0608 - val_loss: 0.0728\n",
      "Epoch 578/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0633 - val_loss: 0.0692\n",
      "Epoch 579/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0610 - val_loss: 0.0726\n",
      "Epoch 580/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0629 - val_loss: 0.0733\n",
      "Epoch 581/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0629 - val_loss: 0.0732\n",
      "Epoch 582/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0626 - val_loss: 0.0715\n",
      "Epoch 583/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0622 - val_loss: 0.0731\n",
      "Epoch 584/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0632 - val_loss: 0.0720\n",
      "Epoch 585/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.1287\n",
      "Epoch 586/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0873 - val_loss: 0.0771\n",
      "Epoch 587/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0659 - val_loss: 0.0701\n",
      "Epoch 588/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0638 - val_loss: 0.0652\n",
      "Epoch 589/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0614 - val_loss: 0.0797\n",
      "Epoch 590/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0683 - val_loss: 0.0663\n",
      "Epoch 591/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.1026\n",
      "Epoch 592/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0769 - val_loss: 0.0870\n",
      "Epoch 593/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0712 - val_loss: 0.0657\n",
      "Epoch 594/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0611 - val_loss: 0.0734\n",
      "Epoch 595/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0651 - val_loss: 0.0716\n",
      "Epoch 596/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0634 - val_loss: 0.0849\n",
      "Epoch 597/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0700 - val_loss: 0.0686\n",
      "Epoch 598/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0624 - val_loss: 0.0706\n",
      "Epoch 599/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0638 - val_loss: 0.0660\n",
      "Epoch 600/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0689\n",
      "\n",
      "Epoch 00600: saving model to ./log_weights/Ide_AE_weights.0600.hdf5\n",
      "Epoch 601/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0633 - val_loss: 0.0713\n",
      "Epoch 602/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0617 - val_loss: 0.0794\n",
      "Epoch 603/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0669 - val_loss: 0.0771\n",
      "Epoch 604/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0629 - val_loss: 0.1226\n",
      "Epoch 605/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0902 - val_loss: 0.0694\n",
      "Epoch 606/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0677\n",
      "Epoch 607/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0730\n",
      "Epoch 608/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0644 - val_loss: 0.0668\n",
      "Epoch 609/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.0754\n",
      "Epoch 610/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0669 - val_loss: 0.0649\n",
      "Epoch 611/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0610 - val_loss: 0.0742\n",
      "Epoch 612/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0659 - val_loss: 0.0707\n",
      "Epoch 613/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0627 - val_loss: 0.0734\n",
      "Epoch 614/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0650 - val_loss: 0.0739\n",
      "Epoch 615/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0622 - val_loss: 0.0804\n",
      "Epoch 616/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0662 - val_loss: 0.0804\n",
      "Epoch 617/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0663 - val_loss: 0.0708\n",
      "Epoch 618/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0614 - val_loss: 0.0792\n",
      "Epoch 619/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0654 - val_loss: 0.0721\n",
      "Epoch 620/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0646 - val_loss: 0.0679\n",
      "Epoch 621/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0610 - val_loss: 0.0795\n",
      "Epoch 622/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0668 - val_loss: 0.0688\n",
      "Epoch 623/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0624 - val_loss: 0.0881\n",
      "Epoch 624/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0706 - val_loss: 0.0682\n",
      "Epoch 625/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0617 - val_loss: 0.0783\n",
      "Epoch 626/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0665 - val_loss: 0.0675\n",
      "Epoch 627/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0621 - val_loss: 0.0684\n",
      "Epoch 628/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0629 - val_loss: 0.0768\n",
      "Epoch 629/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0661 - val_loss: 0.0701\n",
      "Epoch 630/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0620 - val_loss: 0.0727\n",
      "Epoch 631/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0654 - val_loss: 0.0712\n",
      "Epoch 632/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0623 - val_loss: 0.0725\n",
      "Epoch 633/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0621 - val_loss: 0.0901\n",
      "Epoch 634/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0707 - val_loss: 0.0797\n",
      "Epoch 635/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0677 - val_loss: 0.0696\n",
      "Epoch 636/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0615 - val_loss: 0.0855\n",
      "Epoch 637/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0697 - val_loss: 0.0666\n",
      "Epoch 638/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0617 - val_loss: 0.0705\n",
      "Epoch 639/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0635 - val_loss: 0.0707\n",
      "Epoch 640/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0635 - val_loss: 0.0707\n",
      "Epoch 641/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0637 - val_loss: 0.0683\n",
      "Epoch 642/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0749\n",
      "Epoch 643/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0663 - val_loss: 0.0729\n",
      "Epoch 644/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0666 - val_loss: 0.0744\n",
      "Epoch 645/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0657 - val_loss: 0.0745\n",
      "Epoch 646/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0651 - val_loss: 0.0746\n",
      "Epoch 647/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0646 - val_loss: 0.0736\n",
      "Epoch 648/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0648 - val_loss: 0.0705\n",
      "Epoch 649/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0610 - val_loss: 0.0785\n",
      "Epoch 650/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0653 - val_loss: 0.0720\n",
      "Epoch 651/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0809\n",
      "Epoch 652/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0676 - val_loss: 0.0688\n",
      "Epoch 653/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0625 - val_loss: 0.0710\n",
      "Epoch 654/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0648 - val_loss: 0.0663\n",
      "Epoch 655/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0631 - val_loss: 0.0712\n",
      "Epoch 656/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0644 - val_loss: 0.0674\n",
      "Epoch 657/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0624 - val_loss: 0.0701\n",
      "Epoch 658/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0767\n",
      "Epoch 659/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.0692 - val_loss: 0.0749\n",
      "Epoch 660/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0640 - val_loss: 0.0818\n",
      "Epoch 661/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0677 - val_loss: 0.0698\n",
      "Epoch 662/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0623 - val_loss: 0.0699\n",
      "Epoch 663/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0648 - val_loss: 0.0669\n",
      "Epoch 664/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0622 - val_loss: 0.0691\n",
      "Epoch 665/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0631 - val_loss: 0.0682\n",
      "Epoch 666/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0705\n",
      "Epoch 667/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0627 - val_loss: 0.0733\n",
      "Epoch 668/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0637 - val_loss: 0.0780\n",
      "Epoch 669/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0666 - val_loss: 0.0742\n",
      "Epoch 670/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0677 - val_loss: 0.0716\n",
      "Epoch 671/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0648 - val_loss: 0.0740\n",
      "Epoch 672/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0653 - val_loss: 0.0672\n",
      "Epoch 673/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0616 - val_loss: 0.0744\n",
      "Epoch 674/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0653 - val_loss: 0.0688\n",
      "Epoch 675/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0628 - val_loss: 0.0683\n",
      "Epoch 676/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0624 - val_loss: 0.0713\n",
      "Epoch 677/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0645 - val_loss: 0.0703\n",
      "Epoch 678/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0640 - val_loss: 0.0662\n",
      "Epoch 679/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0619 - val_loss: 0.0679\n",
      "Epoch 680/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0729\n",
      "Epoch 681/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0652 - val_loss: 0.0830\n",
      "Epoch 682/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0710 - val_loss: 0.0719\n",
      "Epoch 683/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0624 - val_loss: 0.0834\n",
      "Epoch 684/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0677 - val_loss: 0.0771\n",
      "Epoch 685/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0672 - val_loss: 0.0677\n",
      "Epoch 686/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0634 - val_loss: 0.0671\n",
      "Epoch 687/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0612 - val_loss: 0.0692\n",
      "Epoch 688/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0619 - val_loss: 0.0732\n",
      "Epoch 689/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0653 - val_loss: 0.0805\n",
      "Epoch 690/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0665 - val_loss: 0.0815\n",
      "Epoch 691/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0674 - val_loss: 0.0799\n",
      "Epoch 692/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0681 - val_loss: 0.0653\n",
      "Epoch 693/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0616 - val_loss: 0.0655\n",
      "Epoch 694/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0626 - val_loss: 0.0662\n",
      "Epoch 695/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0680\n",
      "Epoch 696/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0623 - val_loss: 0.0723\n",
      "Epoch 697/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0650 - val_loss: 0.0705\n",
      "Epoch 698/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0619 - val_loss: 0.0739\n",
      "Epoch 699/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0662 - val_loss: 0.0752\n",
      "Epoch 700/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0672 - val_loss: 0.0739\n",
      "\n",
      "Epoch 00700: saving model to ./log_weights/Ide_AE_weights.0700.hdf5\n",
      "Epoch 701/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0652 - val_loss: 0.0721\n",
      "Epoch 702/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0623 - val_loss: 0.0729\n",
      "Epoch 703/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0629 - val_loss: 0.0795\n",
      "Epoch 704/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0665 - val_loss: 0.0745\n",
      "Epoch 705/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0650 - val_loss: 0.1048\n",
      "Epoch 706/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0782 - val_loss: 0.0747\n",
      "Epoch 707/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0655 - val_loss: 0.0664\n",
      "Epoch 708/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0625 - val_loss: 0.0657\n",
      "Epoch 709/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0611 - val_loss: 0.0777\n",
      "Epoch 710/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0662 - val_loss: 0.0851\n",
      "Epoch 711/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0692 - val_loss: 0.0690\n",
      "Epoch 712/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0622 - val_loss: 0.0806\n",
      "Epoch 713/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0678 - val_loss: 0.0701\n",
      "Epoch 714/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0628 - val_loss: 0.0694\n",
      "Epoch 715/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0650 - val_loss: 0.0681\n",
      "Epoch 716/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0620 - val_loss: 0.0738\n",
      "Epoch 717/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0654 - val_loss: 0.0691\n",
      "Epoch 718/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0967\n",
      "Epoch 719/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0736 - val_loss: 0.0901\n",
      "Epoch 720/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0732 - val_loss: 0.0715\n",
      "Epoch 721/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0639 - val_loss: 0.0699\n",
      "Epoch 722/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0617 - val_loss: 0.0712\n",
      "Epoch 723/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0618 - val_loss: 0.0798\n",
      "Epoch 724/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0697 - val_loss: 0.0722\n",
      "Epoch 725/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0649 - val_loss: 0.0714\n",
      "Epoch 726/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0627 - val_loss: 0.0714\n",
      "Epoch 727/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0638 - val_loss: 0.0683\n",
      "Epoch 728/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0705\n",
      "Epoch 729/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0647 - val_loss: 0.0766\n",
      "Epoch 730/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0656 - val_loss: 0.1297\n",
      "Epoch 731/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0902 - val_loss: 0.0703\n",
      "Epoch 732/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0644 - val_loss: 0.0674\n",
      "Epoch 733/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0613 - val_loss: 0.0689\n",
      "Epoch 734/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0610 - val_loss: 0.1125\n",
      "Epoch 735/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0828 - val_loss: 0.0744\n",
      "Epoch 736/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0647 - val_loss: 0.1022\n",
      "Epoch 737/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0775 - val_loss: 0.0691\n",
      "Epoch 738/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0624 - val_loss: 0.0667\n",
      "Epoch 739/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0636 - val_loss: 0.0721\n",
      "Epoch 740/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0660 - val_loss: 0.0654\n",
      "Epoch 741/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0612 - val_loss: 0.0700\n",
      "Epoch 742/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0681 - val_loss: 0.0746\n",
      "Epoch 743/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0649 - val_loss: 0.0738\n",
      "Epoch 744/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0668 - val_loss: 0.0713\n",
      "Epoch 745/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0630 - val_loss: 0.1184\n",
      "Epoch 746/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0875 - val_loss: 0.0691\n",
      "Epoch 747/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0624 - val_loss: 0.0712\n",
      "Epoch 748/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0641 - val_loss: 0.0686\n",
      "Epoch 749/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0628 - val_loss: 0.0685\n",
      "Epoch 750/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.0733\n",
      "Epoch 751/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0658 - val_loss: 0.0942\n",
      "Epoch 752/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0749 - val_loss: 0.0730\n",
      "Epoch 753/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0666 - val_loss: 0.0717\n",
      "Epoch 754/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0638 - val_loss: 0.0708\n",
      "Epoch 755/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0628 - val_loss: 0.0755\n",
      "Epoch 756/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0657 - val_loss: 0.0698\n",
      "Epoch 757/1000\n",
      "36/36 [==============================] - 1s 29ms/step - loss: 0.0628 - val_loss: 0.0718\n",
      "Epoch 758/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0662 - val_loss: 0.0989\n",
      "Epoch 759/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0773 - val_loss: 0.0675\n",
      "Epoch 760/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0628 - val_loss: 0.0665\n",
      "Epoch 761/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0637 - val_loss: 0.0668\n",
      "Epoch 762/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0610 - val_loss: 0.1122\n",
      "Epoch 763/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0825 - val_loss: 0.0741\n",
      "Epoch 764/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0656 - val_loss: 0.1134\n",
      "Epoch 765/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0846 - val_loss: 0.0859\n",
      "Epoch 766/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0697 - val_loss: 0.0791\n",
      "Epoch 767/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0681 - val_loss: 0.0661\n",
      "Epoch 768/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0619 - val_loss: 0.0964\n",
      "Epoch 769/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0764 - val_loss: 0.0678\n",
      "Epoch 770/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0627 - val_loss: 0.0702\n",
      "Epoch 771/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0641 - val_loss: 0.0816\n",
      "Epoch 772/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0685 - val_loss: 0.0929\n",
      "Epoch 773/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0724 - val_loss: 0.1374\n",
      "Epoch 774/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0981 - val_loss: 0.0752\n",
      "Epoch 775/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0670 - val_loss: 0.0665\n",
      "Epoch 776/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0622 - val_loss: 0.1067\n",
      "Epoch 777/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0824 - val_loss: 0.0757\n",
      "Epoch 778/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0667 - val_loss: 0.0698\n",
      "Epoch 779/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0626 - val_loss: 0.1211\n",
      "Epoch 780/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0886 - val_loss: 0.1549\n",
      "Epoch 781/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.1049 - val_loss: 0.0791\n",
      "Epoch 782/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0682 - val_loss: 0.0642\n",
      "Epoch 783/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0652 - val_loss: 0.0653\n",
      "Epoch 784/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0636 - val_loss: 0.0675\n",
      "Epoch 785/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0619 - val_loss: 0.0720\n",
      "Epoch 786/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0685 - val_loss: 0.0768\n",
      "Epoch 787/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0717 - val_loss: 0.0779\n",
      "Epoch 788/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0744 - val_loss: 0.0785\n",
      "Epoch 789/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0689 - val_loss: 0.0785\n",
      "Epoch 790/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0671 - val_loss: 0.0834\n",
      "Epoch 791/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0704 - val_loss: 0.0669\n",
      "Epoch 792/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0609 - val_loss: 0.0928\n",
      "Epoch 793/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0751 - val_loss: 0.0708\n",
      "Epoch 794/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0645 - val_loss: 0.0932\n",
      "Epoch 795/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0754 - val_loss: 0.0671\n",
      "Epoch 796/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0624 - val_loss: 0.0727\n",
      "Epoch 797/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0657 - val_loss: 0.0686\n",
      "Epoch 798/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0663 - val_loss: 0.0919\n",
      "Epoch 799/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0743 - val_loss: 0.0702\n",
      "Epoch 800/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0631 - val_loss: 0.0993\n",
      "\n",
      "Epoch 00800: saving model to ./log_weights/Ide_AE_weights.0800.hdf5\n",
      "Epoch 801/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0785 - val_loss: 0.0694\n",
      "Epoch 802/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0624 - val_loss: 0.0720\n",
      "Epoch 803/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0636 - val_loss: 0.0924\n",
      "Epoch 804/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0746 - val_loss: 0.0721\n",
      "Epoch 805/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0644 - val_loss: 0.0754\n",
      "Epoch 806/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0653 - val_loss: 0.0891\n",
      "Epoch 807/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0736 - val_loss: 0.1206\n",
      "Epoch 808/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0885 - val_loss: 0.0690\n",
      "Epoch 809/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0646 - val_loss: 0.1159\n",
      "Epoch 810/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0871 - val_loss: 0.0775\n",
      "Epoch 811/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0690 - val_loss: 0.0678\n",
      "Epoch 812/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0631 - val_loss: 0.0722\n",
      "Epoch 813/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0665 - val_loss: 0.0749\n",
      "Epoch 814/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0763 - val_loss: 0.1663\n",
      "Epoch 815/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.1122 - val_loss: 0.0920\n",
      "Epoch 816/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0744 - val_loss: 0.0726\n",
      "Epoch 817/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0637 - val_loss: 0.0788\n",
      "Epoch 818/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0677 - val_loss: 0.0851\n",
      "Epoch 819/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0717 - val_loss: 0.1065\n",
      "Epoch 820/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0826 - val_loss: 0.0687\n",
      "Epoch 821/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0652 - val_loss: 0.0758\n",
      "Epoch 822/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0669 - val_loss: 0.0920\n",
      "Epoch 823/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0746 - val_loss: 0.0806\n",
      "Epoch 824/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0703 - val_loss: 0.0777\n",
      "Epoch 825/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0681 - val_loss: 0.0899\n",
      "Epoch 826/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0738 - val_loss: 0.1276\n",
      "Epoch 827/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0938 - val_loss: 0.0903\n",
      "Epoch 828/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0741 - val_loss: 0.0811\n",
      "Epoch 829/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0692 - val_loss: 0.1270\n",
      "Epoch 830/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0956 - val_loss: 0.0696\n",
      "Epoch 831/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0646 - val_loss: 0.1104\n",
      "Epoch 832/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0846 - val_loss: 0.1453\n",
      "Epoch 833/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1016 - val_loss: 0.1016\n",
      "Epoch 834/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0848 - val_loss: 0.0719\n",
      "Epoch 835/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0625 - val_loss: 0.1118\n",
      "Epoch 836/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0879 - val_loss: 0.0801\n",
      "Epoch 837/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0750 - val_loss: 0.0763\n",
      "Epoch 838/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0664 - val_loss: 0.0903\n",
      "Epoch 839/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0765 - val_loss: 0.0912\n",
      "Epoch 840/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0808 - val_loss: 0.0735\n",
      "Epoch 841/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0678 - val_loss: 0.0728\n",
      "Epoch 842/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0709 - val_loss: 0.0766\n",
      "Epoch 843/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0662 - val_loss: 0.0982\n",
      "Epoch 844/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0765 - val_loss: 0.0812\n",
      "Epoch 845/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0751 - val_loss: 0.0778\n",
      "Epoch 846/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0696 - val_loss: 0.0916\n",
      "Epoch 847/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0772 - val_loss: 0.0857\n",
      "Epoch 848/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1036 - val_loss: 0.0850\n",
      "Epoch 849/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0741 - val_loss: 0.0924\n",
      "Epoch 850/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0797 - val_loss: 0.0747\n",
      "Epoch 851/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0668 - val_loss: 0.0822\n",
      "Epoch 852/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0717 - val_loss: 0.0810\n",
      "Epoch 853/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0742 - val_loss: 0.0841\n",
      "Epoch 854/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0796 - val_loss: 0.0851\n",
      "Epoch 855/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0711 - val_loss: 0.0747\n",
      "Epoch 856/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0657 - val_loss: 0.0813\n",
      "Epoch 857/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0729 - val_loss: 0.0772\n",
      "Epoch 858/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1042 - val_loss: 0.0838\n",
      "Epoch 859/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0762 - val_loss: 0.3004\n",
      "Epoch 860/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1843 - val_loss: 0.0882\n",
      "Epoch 861/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0889 - val_loss: 0.0942\n",
      "Epoch 862/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0907 - val_loss: 0.0815\n",
      "Epoch 863/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0734 - val_loss: 0.0771\n",
      "Epoch 864/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0699 - val_loss: 0.0775\n",
      "Epoch 865/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0822 - val_loss: 0.0784\n",
      "Epoch 866/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0734 - val_loss: 0.0849\n",
      "Epoch 867/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0819 - val_loss: 0.1399\n",
      "Epoch 868/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0981 - val_loss: 0.2289\n",
      "Epoch 869/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.1426 - val_loss: 0.0837\n",
      "Epoch 870/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0720 - val_loss: 0.0800\n",
      "Epoch 871/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0744 - val_loss: 0.0882\n",
      "Epoch 872/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0793 - val_loss: 0.0710\n",
      "Epoch 873/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0760 - val_loss: 0.0833\n",
      "Epoch 874/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0735 - val_loss: 0.0745\n",
      "Epoch 875/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0701 - val_loss: 0.0823\n",
      "Epoch 876/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0941 - val_loss: 0.0740\n",
      "Epoch 877/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0675 - val_loss: 0.0843\n",
      "Epoch 878/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1094 - val_loss: 0.0713\n",
      "Epoch 879/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0667 - val_loss: 0.0737\n",
      "Epoch 880/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0690 - val_loss: 0.0764\n",
      "Epoch 881/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0670 - val_loss: 0.1291\n",
      "Epoch 882/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0921 - val_loss: 0.1264\n",
      "Epoch 883/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0910 - val_loss: 0.0951\n",
      "Epoch 884/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0741 - val_loss: 0.0778\n",
      "Epoch 885/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0668 - val_loss: 0.0710\n",
      "Epoch 886/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0781 - val_loss: 0.0815\n",
      "Epoch 887/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0838 - val_loss: 0.0799\n",
      "Epoch 888/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1028 - val_loss: 0.0784\n",
      "Epoch 889/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0957 - val_loss: 0.1042\n",
      "Epoch 890/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0875 - val_loss: 0.4003\n",
      "Epoch 891/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.2665 - val_loss: 0.0918\n",
      "Epoch 892/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0833 - val_loss: 0.0843\n",
      "Epoch 893/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0708 - val_loss: 0.0838\n",
      "Epoch 894/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0774 - val_loss: 0.0804\n",
      "Epoch 895/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0710 - val_loss: 0.0758\n",
      "Epoch 896/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0706 - val_loss: 0.0736\n",
      "Epoch 897/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0625 - val_loss: 0.0783\n",
      "Epoch 898/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0673 - val_loss: 0.0686\n",
      "Epoch 899/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0614 - val_loss: 0.0782\n",
      "Epoch 900/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0665 - val_loss: 0.0995\n",
      "\n",
      "Epoch 00900: saving model to ./log_weights/Ide_AE_weights.0900.hdf5\n",
      "Epoch 901/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0783 - val_loss: 0.0734\n",
      "Epoch 902/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0648 - val_loss: 0.0769\n",
      "Epoch 903/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0698 - val_loss: 0.0743\n",
      "Epoch 904/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0634 - val_loss: 0.0813\n",
      "Epoch 905/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0774 - val_loss: 0.0803\n",
      "Epoch 906/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0745 - val_loss: 0.0726\n",
      "Epoch 907/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0639 - val_loss: 0.0697\n",
      "Epoch 908/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0604 - val_loss: 0.1156\n",
      "Epoch 909/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0885 - val_loss: 0.0747\n",
      "Epoch 910/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0629 - val_loss: 0.0681\n",
      "Epoch 911/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0657 - val_loss: 0.0941\n",
      "Epoch 912/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1043 - val_loss: 0.0807\n",
      "Epoch 913/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0645 - val_loss: 0.1115\n",
      "Epoch 914/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0806 - val_loss: 0.0899\n",
      "Epoch 915/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0685 - val_loss: 0.0802\n",
      "Epoch 916/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0697 - val_loss: 0.0704\n",
      "Epoch 917/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0605 - val_loss: 0.0878\n",
      "Epoch 918/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0710 - val_loss: 0.1030\n",
      "Epoch 919/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0748 - val_loss: 0.0755\n",
      "Epoch 920/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0958 - val_loss: 0.1013\n",
      "Epoch 921/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0825 - val_loss: 0.0701\n",
      "Epoch 922/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0656 - val_loss: 0.0827\n",
      "Epoch 923/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0659 - val_loss: 0.0728\n",
      "Epoch 924/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0620 - val_loss: 0.0757\n",
      "Epoch 925/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0906 - val_loss: 0.0791\n",
      "Epoch 926/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0646 - val_loss: 0.0694\n",
      "Epoch 927/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0588 - val_loss: 0.0727\n",
      "Epoch 928/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0674 - val_loss: 0.0954\n",
      "Epoch 929/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0787 - val_loss: 0.0734\n",
      "Epoch 930/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0646 - val_loss: 0.1273\n",
      "Epoch 931/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0876 - val_loss: 0.1255\n",
      "Epoch 932/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0862 - val_loss: 0.0939\n",
      "Epoch 933/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0692 - val_loss: 0.0730\n",
      "Epoch 934/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0605 - val_loss: 0.0852\n",
      "Epoch 935/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.1559 - val_loss: 0.0777\n",
      "Epoch 936/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0622 - val_loss: 0.0665\n",
      "Epoch 937/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0600 - val_loss: 0.0745\n",
      "Epoch 938/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0652 - val_loss: 0.0658\n",
      "Epoch 939/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0581 - val_loss: 0.0697\n",
      "Epoch 940/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0594 - val_loss: 0.1061\n",
      "Epoch 941/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0785 - val_loss: 0.1504\n",
      "Epoch 942/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1026 - val_loss: 0.2489\n",
      "Epoch 943/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1632 - val_loss: 0.0785\n",
      "Epoch 944/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0643 - val_loss: 0.0759\n",
      "Epoch 945/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0639 - val_loss: 0.0761\n",
      "Epoch 946/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0626 - val_loss: 0.0719\n",
      "Epoch 947/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0743 - val_loss: 0.0760\n",
      "Epoch 948/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0614 - val_loss: 0.0659\n",
      "Epoch 949/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0575 - val_loss: 0.0737\n",
      "Epoch 950/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0616 - val_loss: 0.0969\n",
      "Epoch 951/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0737 - val_loss: 0.0668\n",
      "Epoch 952/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0605 - val_loss: 0.0701\n",
      "Epoch 953/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0631 - val_loss: 0.0646\n",
      "Epoch 954/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0607 - val_loss: 0.0652\n",
      "Epoch 955/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0562 - val_loss: 0.0720\n",
      "Epoch 956/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0603 - val_loss: 0.0757\n",
      "Epoch 957/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0640 - val_loss: 0.0627\n",
      "Epoch 958/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0601 - val_loss: 0.0642\n",
      "Epoch 959/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0554 - val_loss: 0.0809\n",
      "Epoch 960/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0653 - val_loss: 0.1850\n",
      "Epoch 961/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.1212 - val_loss: 0.0842\n",
      "Epoch 962/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0689 - val_loss: 0.0808\n",
      "Epoch 963/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0634 - val_loss: 0.0809\n",
      "Epoch 964/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0985 - val_loss: 0.0837\n",
      "Epoch 965/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0650 - val_loss: 0.0634\n",
      "Epoch 966/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0562 - val_loss: 0.1312\n",
      "Epoch 967/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0946 - val_loss: 0.0721\n",
      "Epoch 968/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0604 - val_loss: 0.0675\n",
      "Epoch 969/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0574 - val_loss: 0.0754\n",
      "Epoch 970/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0616 - val_loss: 0.0687\n",
      "Epoch 971/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0574 - val_loss: 0.1644\n",
      "Epoch 972/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.1092 - val_loss: 0.0691\n",
      "Epoch 973/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0763 - val_loss: 0.0706\n",
      "Epoch 974/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0695 - val_loss: 0.0871\n",
      "Epoch 975/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0706 - val_loss: 0.0975\n",
      "Epoch 976/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0759 - val_loss: 0.0654\n",
      "Epoch 977/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0587 - val_loss: 0.0821\n",
      "Epoch 978/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0643 - val_loss: 0.0774\n",
      "Epoch 979/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0743 - val_loss: 0.0629\n",
      "Epoch 980/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0551 - val_loss: 0.0791\n",
      "Epoch 981/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0912 - val_loss: 0.0667\n",
      "Epoch 982/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0568 - val_loss: 0.0641\n",
      "Epoch 983/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0575 - val_loss: 0.0732\n",
      "Epoch 984/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0765 - val_loss: 0.0680\n",
      "Epoch 985/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0608 - val_loss: 0.0616\n",
      "Epoch 986/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0532 - val_loss: 0.0879\n",
      "Epoch 987/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0700 - val_loss: 0.0729\n",
      "Epoch 988/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0638 - val_loss: 0.0672\n",
      "Epoch 989/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0555 - val_loss: 0.0654\n",
      "Epoch 990/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0566 - val_loss: 0.0708\n",
      "Epoch 991/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0673 - val_loss: 0.0727\n",
      "Epoch 992/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0635 - val_loss: 0.0614\n",
      "Epoch 993/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0540 - val_loss: 0.1975\n",
      "Epoch 994/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.1283 - val_loss: 0.0688\n",
      "Epoch 995/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0615 - val_loss: 0.0735\n",
      "Epoch 996/1000\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.0668 - val_loss: 0.0854\n",
      "Epoch 997/1000\n",
      "36/36 [==============================] - 1s 27ms/step - loss: 0.0664 - val_loss: 0.0670\n",
      "Epoch 998/1000\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.0571 - val_loss: 0.0632\n",
      "Epoch 999/1000\n",
      "36/36 [==============================] - 1s 25ms/step - loss: 0.0550 - val_loss: 0.0662\n",
      "Epoch 1000/1000\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.0557 - val_loss: 0.1379\n",
      "\n",
      "Epoch 01000: saving model to ./log_weights/Ide_AE_weights.1000.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint=ModelCheckpoint('./log_weights/Ide_AE_weights.{epoch:04d}.hdf5',period=100,save_weights_only=True,verbose=1)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(Ide_AE.layers[1].get_weights()))\n",
    "\n",
    "Ide_AE_history = Ide_AE.fit(x_train, x_train,\\\n",
    "                            epochs=epochs_number,\\\n",
    "                            batch_size=batch_size_value,\\\n",
    "                            shuffle=True,\\\n",
    "                            validation_data=(x_validate,x_validate),\\\n",
    "                            callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = Ide_AE_history.history['loss']\n",
    "val_loss = Ide_AE_history.history['val_loss']\n",
    "\n",
    "epochs = range(epochs_number)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for one-to-one map layer 0.06042176581262098\n"
     ]
    }
   ],
   "source": [
    "p_data=Ide_AE.predict(x_test)\n",
    "numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)-x_test,2))/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "key_number=64\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_number=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features=F.top_k_keepWeights_1(Ide_AE.get_layer(index=1).get_weights()[0],key_number)\n",
    "\n",
    "selected_position_list=np.where(key_features>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Training accuracy 1.0\n",
      "Testing accuracy 0.9\n",
      "Testing accuracy 0.9\n"
     ]
    }
   ],
   "source": [
    "train_feature=C_train_x\n",
    "train_label=C_train_y\n",
    "test_feature=C_test_x\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 64)\n",
      "(10, 64)\n",
      "Training accuracy 1.0\n",
      "Training accuracy 1.0\n",
      "Testing accuracy 0.8\n",
      "Testing accuracy 0.8\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 64)\n",
      "(10, 64)\n",
      "0.054244940898892056\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(C_train_selected_x.shape)\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(C_test_selected_x.shape)\n",
    "\n",
    "\n",
    "train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
