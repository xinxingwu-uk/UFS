{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import random\n",
    "import scipy.sparse as sparse\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./Dataset/Prostate_GE.mat\"\n",
    "Data = scipy.io.loadmat(data_path)\n",
    "\n",
    "data_arr=Data['X']\n",
    "label_arr=Data['Y'][:, 0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=MinMaxScaler(feature_range=(0,1)).fit_transform(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (72, 5966)\n",
      "Shape of x_validate: (9, 5966)\n",
      "Shape of x_test: (21, 5966)\n",
      "Shape of y_train: (72,)\n",
      "Shape of y_validate: (9,)\n",
      "Shape of y_test: (21,)\n",
      "Shape of C_train_x: (81, 5966)\n",
      "Shape of C_train_y: (81,)\n",
      "Shape of C_test_x: (21, 5966)\n",
      "Shape of C_test_y: (21,)\n"
     ]
    }
   ],
   "source": [
    "C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(Data,label_arr,test_size=0.2,random_state=seed)\n",
    "x_train,x_validate,y_train_onehot,y_validate_onehot= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=seed)\n",
    "\n",
    "x_test=C_test_x\n",
    "y_test_onehot=C_test_y\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_validate: ' + str(x_validate.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train_onehot.shape))\n",
    "print('Shape of y_validate: ' + str(y_validate_onehot.shape))\n",
    "print('Shape of y_test: ' + str(y_test_onehot.shape))\n",
    "\n",
    "print('Shape of C_train_x: ' + str(C_train_x.shape)) \n",
    "print('Shape of C_train_y: ' + str(C_train_y.shape)) \n",
    "print('Shape of C_test_x: ' + str(C_test_x.shape)) \n",
    "print('Shape of C_test_y: ' + str(C_test_y.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "class Feature_Select_Layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, l1_lambda, **kwargs):\n",
    "        super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.l1_lambda=l1_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0., maxval=1.),\n",
    "                                      trainable=True,\n",
    "                                      regularizer=regularizers.l1(self.l1_lambda),\n",
    "                                      constraint=constraints.NonNeg())\n",
    "        super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, selection=False,k=36):\n",
    "        kernel=self.kernel        \n",
    "        if selection:\n",
    "            kernel_=K.transpose(kernel)\n",
    "            print(kernel_.shape)\n",
    "            kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "            kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "        return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                         p_encoding_dim=50,\\\n",
    "                         p_learning_rate= 1E-3,\\\n",
    "                         p_l1_lambda=0.1):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             l1_lambda=p_l1_lambda,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,latent_encoder_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "batch_size_value=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1.1 Identity Autoencoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Autoencoder Structure-------------------------------------\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye (None, 5966)              0         \n",
      "_________________________________________________________________\n",
      "feature_selection (Feature_S (None, 5966)              5966      \n",
      "_________________________________________________________________\n",
      "autoencoder_hidden_layer (De (None, 64)                381888    \n",
      "_________________________________________________________________\n",
      "autoencoder_output (Dense)   (None, 5966)              387790    \n",
      "=================================================================\n",
      "Total params: 775,644\n",
      "Trainable params: 775,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ide_AE,\\\n",
    "latent_encoder_score_Ide_AE=Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                                 p_encoding_dim=64,\\\n",
    "                                                 p_learning_rate= 1E-2,\\\n",
    "                                                 p_l1_lambda=l1_lambda)\n",
    "\n",
    "file_name=\"./log/AgnoSS.png\"\n",
    "plot_model(Ide_AE, to_file=file_name,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 72 samples, validate on 9 samples\n",
      "Epoch 1/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 2872.4829 - val_loss: 2701.5188\n",
      "Epoch 2/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 2604.2337 - val_loss: 2429.6616\n",
      "Epoch 3/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 2335.7083 - val_loss: 2168.4138\n",
      "Epoch 4/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 2078.9528 - val_loss: 1920.0164\n",
      "Epoch 5/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 1835.6496 - val_loss: 1685.7527\n",
      "Epoch 6/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 1606.3252 - val_loss: 1465.9587\n",
      "Epoch 7/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 1392.2040 - val_loss: 1262.1776\n",
      "Epoch 8/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 1194.0697 - val_loss: 1073.8580\n",
      "Epoch 9/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 1010.9049 - val_loss: 900.5706\n",
      "Epoch 10/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 843.5072 - val_loss: 743.4564\n",
      "Epoch 11/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 691.5366 - val_loss: 601.4049\n",
      "Epoch 12/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 555.2513 - val_loss: 474.8538\n",
      "Epoch 13/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 433.7721 - val_loss: 362.7848\n",
      "Epoch 14/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 327.1068 - val_loss: 265.7198\n",
      "Epoch 15/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 235.0606 - val_loss: 183.1207\n",
      "Epoch 16/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 158.1844 - val_loss: 116.4870\n",
      "Epoch 17/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 96.9867 - val_loss: 64.9701\n",
      "Epoch 18/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 50.7120 - val_loss: 28.4904\n",
      "Epoch 19/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 19.6537 - val_loss: 7.0770\n",
      "Epoch 20/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 3.4280 - val_loss: 0.0774\n",
      "Epoch 21/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0563 - val_loss: 0.0738\n",
      "Epoch 22/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0596 - val_loss: 0.0742\n",
      "Epoch 23/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0557 - val_loss: 0.0730\n",
      "Epoch 24/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0545 - val_loss: 0.0722\n",
      "Epoch 25/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0708\n",
      "Epoch 26/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0540 - val_loss: 0.0713\n",
      "Epoch 27/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0539 - val_loss: 0.0710\n",
      "Epoch 28/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0715\n",
      "Epoch 29/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0544 - val_loss: 0.0712\n",
      "Epoch 30/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0721\n",
      "Epoch 31/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0537 - val_loss: 0.0710\n",
      "Epoch 32/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0542 - val_loss: 0.0704\n",
      "Epoch 33/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0718\n",
      "Epoch 34/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0540 - val_loss: 0.0720\n",
      "Epoch 35/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0711\n",
      "Epoch 36/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0539 - val_loss: 0.0707\n",
      "Epoch 37/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0537 - val_loss: 0.0709\n",
      "Epoch 38/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0538 - val_loss: 0.0713\n",
      "Epoch 39/1000\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.0542 - val_loss: 0.0711\n",
      "Epoch 40/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0541 - val_loss: 0.0712\n",
      "Epoch 41/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0540 - val_loss: 0.0713\n",
      "Epoch 42/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0539 - val_loss: 0.0715\n",
      "Epoch 43/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0539 - val_loss: 0.0725\n",
      "Epoch 44/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0539 - val_loss: 0.0711\n",
      "Epoch 45/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0549 - val_loss: 0.0706\n",
      "Epoch 46/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0538 - val_loss: 0.0715\n",
      "Epoch 47/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0730\n",
      "Epoch 48/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0710\n",
      "Epoch 49/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0538 - val_loss: 0.0708\n",
      "Epoch 50/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0538 - val_loss: 0.0716\n",
      "Epoch 51/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0723\n",
      "Epoch 52/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0541 - val_loss: 0.0716\n",
      "Epoch 53/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0538 - val_loss: 0.0714\n",
      "Epoch 54/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0541 - val_loss: 0.0707\n",
      "Epoch 55/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0543 - val_loss: 0.0717\n",
      "Epoch 56/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0717\n",
      "Epoch 57/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0712\n",
      "Epoch 58/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0540 - val_loss: 0.0718\n",
      "Epoch 59/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0711\n",
      "Epoch 60/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0539 - val_loss: 0.0717\n",
      "Epoch 61/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0706\n",
      "Epoch 62/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0539 - val_loss: 0.0713\n",
      "Epoch 63/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0740\n",
      "Epoch 64/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0716\n",
      "Epoch 65/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0712\n",
      "Epoch 66/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0711\n",
      "Epoch 67/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0547 - val_loss: 0.0741\n",
      "Epoch 68/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0712\n",
      "Epoch 69/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0727\n",
      "Epoch 70/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0554 - val_loss: 0.0707\n",
      "Epoch 71/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0539 - val_loss: 0.0728\n",
      "Epoch 72/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0546 - val_loss: 0.0735\n",
      "Epoch 73/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0552 - val_loss: 0.0707\n",
      "Epoch 74/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0736\n",
      "Epoch 75/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0712\n",
      "Epoch 76/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0539 - val_loss: 0.0713\n",
      "Epoch 77/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0707\n",
      "Epoch 78/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0725\n",
      "Epoch 79/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0548 - val_loss: 0.0715\n",
      "Epoch 80/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0539 - val_loss: 0.0710\n",
      "Epoch 81/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0540 - val_loss: 0.0725\n",
      "Epoch 82/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0711\n",
      "Epoch 83/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0540 - val_loss: 0.0722\n",
      "Epoch 84/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0708\n",
      "Epoch 85/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0719\n",
      "Epoch 86/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0711\n",
      "Epoch 87/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0539 - val_loss: 0.0710\n",
      "Epoch 88/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0540 - val_loss: 0.0720\n",
      "Epoch 89/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0541 - val_loss: 0.0710\n",
      "Epoch 90/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0706\n",
      "Epoch 91/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0538 - val_loss: 0.0714\n",
      "Epoch 92/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0544 - val_loss: 0.0738\n",
      "Epoch 93/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0549 - val_loss: 0.0712\n",
      "Epoch 94/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0539 - val_loss: 0.0723\n",
      "Epoch 95/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0720\n",
      "Epoch 96/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0713\n",
      "Epoch 97/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0710\n",
      "Epoch 98/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0720\n",
      "Epoch 99/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0712\n",
      "Epoch 100/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00100: saving model to ./log_weights/Ide_AE_weights.0100.hdf5\n",
      "Epoch 101/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0711\n",
      "Epoch 102/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0550 - val_loss: 0.0711\n",
      "Epoch 103/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0549 - val_loss: 0.0728\n",
      "Epoch 104/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0710\n",
      "Epoch 105/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0539 - val_loss: 0.0712\n",
      "Epoch 106/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0713\n",
      "Epoch 107/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0543 - val_loss: 0.0793\n",
      "Epoch 108/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0555 - val_loss: 0.0707\n",
      "Epoch 109/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0707\n",
      "Epoch 110/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0719\n",
      "Epoch 111/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0721\n",
      "Epoch 112/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0542 - val_loss: 0.0715\n",
      "Epoch 113/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0542 - val_loss: 0.0710\n",
      "Epoch 114/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0715\n",
      "Epoch 115/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0715\n",
      "Epoch 116/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0555 - val_loss: 0.0738\n",
      "Epoch 117/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0548 - val_loss: 0.0705\n",
      "Epoch 118/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0539 - val_loss: 0.0720\n",
      "Epoch 119/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0542 - val_loss: 0.0715\n",
      "Epoch 120/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0711\n",
      "Epoch 121/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0539 - val_loss: 0.0718\n",
      "Epoch 122/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0547 - val_loss: 0.0710\n",
      "Epoch 123/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0724\n",
      "Epoch 124/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0744\n",
      "Epoch 125/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0551 - val_loss: 0.0704\n",
      "Epoch 126/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0713\n",
      "Epoch 127/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0720\n",
      "Epoch 128/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0752\n",
      "Epoch 129/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0710\n",
      "Epoch 130/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0541 - val_loss: 0.0715\n",
      "Epoch 131/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0548 - val_loss: 0.0722\n",
      "Epoch 132/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0710\n",
      "Epoch 133/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0715\n",
      "Epoch 134/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0743\n",
      "Epoch 135/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0545 - val_loss: 0.0706\n",
      "Epoch 136/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0710\n",
      "Epoch 137/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0735\n",
      "Epoch 138/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0714\n",
      "Epoch 139/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0541 - val_loss: 0.0712\n",
      "Epoch 140/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0708\n",
      "Epoch 141/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0711\n",
      "Epoch 142/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0718\n",
      "Epoch 143/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0722\n",
      "Epoch 144/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0707\n",
      "Epoch 145/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0732\n",
      "Epoch 146/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0709\n",
      "Epoch 147/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0711\n",
      "Epoch 148/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0719\n",
      "Epoch 149/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0709\n",
      "Epoch 150/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0710\n",
      "Epoch 151/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0713\n",
      "Epoch 152/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0542 - val_loss: 0.0726\n",
      "Epoch 153/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0711\n",
      "Epoch 154/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0718\n",
      "Epoch 155/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0718\n",
      "Epoch 156/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0542 - val_loss: 0.0717\n",
      "Epoch 157/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0709\n",
      "Epoch 158/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0540 - val_loss: 0.0711\n",
      "Epoch 159/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0542 - val_loss: 0.0731\n",
      "Epoch 160/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0709\n",
      "Epoch 161/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0539 - val_loss: 0.0719\n",
      "Epoch 162/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0547 - val_loss: 0.0714\n",
      "Epoch 163/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0716\n",
      "Epoch 164/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0541 - val_loss: 0.0724\n",
      "Epoch 165/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0709\n",
      "Epoch 166/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0541 - val_loss: 0.0718\n",
      "Epoch 167/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0720\n",
      "Epoch 168/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0710\n",
      "Epoch 169/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0713\n",
      "Epoch 170/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0708\n",
      "Epoch 171/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0541 - val_loss: 0.0720\n",
      "Epoch 172/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0709\n",
      "Epoch 173/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0542 - val_loss: 0.0722\n",
      "Epoch 174/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0711\n",
      "Epoch 175/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0721\n",
      "Epoch 176/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0546 - val_loss: 0.0710\n",
      "Epoch 177/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0713\n",
      "Epoch 178/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0540 - val_loss: 0.0723\n",
      "Epoch 179/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0734\n",
      "Epoch 180/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0712\n",
      "Epoch 181/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0709\n",
      "Epoch 182/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0539 - val_loss: 0.0723\n",
      "Epoch 183/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0550 - val_loss: 0.0742\n",
      "Epoch 184/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0710\n",
      "Epoch 185/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0542 - val_loss: 0.0710\n",
      "Epoch 186/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0719\n",
      "Epoch 187/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0546 - val_loss: 0.0713\n",
      "Epoch 188/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0541 - val_loss: 0.0711\n",
      "Epoch 189/1000\n",
      "72/72 [==============================] - 3s 47ms/step - loss: 0.0541 - val_loss: 0.0745\n",
      "Epoch 190/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0548 - val_loss: 0.0710\n",
      "Epoch 191/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0715\n",
      "Epoch 192/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0725\n",
      "Epoch 193/1000\n",
      "72/72 [==============================] - 4s 53ms/step - loss: 0.0545 - val_loss: 0.0722\n",
      "Epoch 194/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0710\n",
      "Epoch 195/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0540 - val_loss: 0.0718\n",
      "Epoch 196/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0711\n",
      "Epoch 197/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0541 - val_loss: 0.0714\n",
      "Epoch 198/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0719\n",
      "Epoch 199/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0720\n",
      "Epoch 200/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0546 - val_loss: 0.0709\n",
      "\n",
      "Epoch 00200: saving model to ./log_weights/Ide_AE_weights.0200.hdf5\n",
      "Epoch 201/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0722\n",
      "Epoch 202/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0735\n",
      "Epoch 203/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0542 - val_loss: 0.0714\n",
      "Epoch 204/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0544 - val_loss: 0.0709\n",
      "Epoch 205/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0548 - val_loss: 0.0710\n",
      "Epoch 206/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0542 - val_loss: 0.0737\n",
      "Epoch 207/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0724\n",
      "Epoch 208/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0728\n",
      "Epoch 209/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0546 - val_loss: 0.0712\n",
      "Epoch 210/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0738\n",
      "Epoch 211/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0710\n",
      "Epoch 212/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0712\n",
      "Epoch 213/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0547 - val_loss: 0.0732\n",
      "Epoch 214/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0717\n",
      "Epoch 215/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0715\n",
      "Epoch 216/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0710\n",
      "Epoch 217/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0716\n",
      "Epoch 218/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0542 - val_loss: 0.0726\n",
      "Epoch 219/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0543 - val_loss: 0.0716\n",
      "Epoch 220/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0551 - val_loss: 0.0712\n",
      "Epoch 221/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0541 - val_loss: 0.0711\n",
      "Epoch 222/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0708\n",
      "Epoch 223/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0716\n",
      "Epoch 224/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0717\n",
      "Epoch 225/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0731\n",
      "Epoch 226/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0544 - val_loss: 0.0751\n",
      "Epoch 227/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0550 - val_loss: 0.0711\n",
      "Epoch 228/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0706\n",
      "Epoch 229/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0709\n",
      "Epoch 230/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0759\n",
      "Epoch 231/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0552 - val_loss: 0.0713\n",
      "Epoch 232/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0541 - val_loss: 0.0715\n",
      "Epoch 233/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0732\n",
      "Epoch 234/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0544 - val_loss: 0.0724\n",
      "Epoch 235/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0710\n",
      "Epoch 236/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0723\n",
      "Epoch 237/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0737\n",
      "Epoch 238/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0717\n",
      "Epoch 239/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0542 - val_loss: 0.0732\n",
      "Epoch 240/1000\n",
      "72/72 [==============================] - 3s 44ms/step - loss: 0.0547 - val_loss: 0.0711\n",
      "Epoch 241/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0544 - val_loss: 0.0714\n",
      "Epoch 242/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0714\n",
      "Epoch 243/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0544 - val_loss: 0.0731\n",
      "Epoch 244/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0544 - val_loss: 0.0738\n",
      "Epoch 245/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0719\n",
      "Epoch 246/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0545 - val_loss: 0.0713\n",
      "Epoch 247/1000\n",
      "72/72 [==============================] - 3s 46ms/step - loss: 0.0543 - val_loss: 0.0714\n",
      "Epoch 248/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0543 - val_loss: 0.0726\n",
      "Epoch 249/1000\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.0546 - val_loss: 0.0713\n",
      "Epoch 250/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0544 - val_loss: 0.0729\n",
      "Epoch 251/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0550 - val_loss: 0.0719\n",
      "Epoch 252/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0706\n",
      "Epoch 253/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0715\n",
      "Epoch 254/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0553 - val_loss: 0.0713\n",
      "Epoch 255/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0542 - val_loss: 0.0752\n",
      "Epoch 256/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0549 - val_loss: 0.0718\n",
      "Epoch 257/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0729\n",
      "Epoch 258/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0712\n",
      "Epoch 259/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0709\n",
      "Epoch 260/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0540 - val_loss: 0.0737\n",
      "Epoch 261/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0548 - val_loss: 0.0714\n",
      "Epoch 262/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0542 - val_loss: 0.0712\n",
      "Epoch 263/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0730\n",
      "Epoch 264/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0718\n",
      "Epoch 265/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0544 - val_loss: 0.0716\n",
      "Epoch 266/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0548 - val_loss: 0.0708\n",
      "Epoch 267/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0763\n",
      "Epoch 268/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0558 - val_loss: 0.0708\n",
      "Epoch 269/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0739\n",
      "Epoch 270/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0736\n",
      "Epoch 271/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0721\n",
      "Epoch 272/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0545 - val_loss: 0.0714\n",
      "Epoch 273/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0725\n",
      "Epoch 274/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0547 - val_loss: 0.0709\n",
      "Epoch 275/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0719\n",
      "Epoch 276/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0549 - val_loss: 0.0724\n",
      "Epoch 277/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0712\n",
      "Epoch 278/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0714\n",
      "Epoch 279/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0546 - val_loss: 0.0712\n",
      "Epoch 280/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0543 - val_loss: 0.0736\n",
      "Epoch 281/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0712\n",
      "Epoch 282/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0715\n",
      "Epoch 283/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0549 - val_loss: 0.0714\n",
      "Epoch 284/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0731\n",
      "Epoch 285/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0731\n",
      "Epoch 286/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0715\n",
      "Epoch 287/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0543 - val_loss: 0.0716\n",
      "Epoch 288/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0749\n",
      "Epoch 289/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0554 - val_loss: 0.0707\n",
      "Epoch 290/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0724\n",
      "Epoch 291/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0545 - val_loss: 0.0722\n",
      "Epoch 292/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0542 - val_loss: 0.0716\n",
      "Epoch 293/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0713\n",
      "Epoch 294/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0708\n",
      "Epoch 295/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0741\n",
      "Epoch 296/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0550 - val_loss: 0.0707\n",
      "Epoch 297/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0717\n",
      "Epoch 298/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0721\n",
      "Epoch 299/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0712\n",
      "Epoch 300/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0712\n",
      "\n",
      "Epoch 00300: saving model to ./log_weights/Ide_AE_weights.0300.hdf5\n",
      "Epoch 301/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0540 - val_loss: 0.0724\n",
      "Epoch 302/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0710\n",
      "Epoch 303/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0544 - val_loss: 0.0713\n",
      "Epoch 304/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0552 - val_loss: 0.0712\n",
      "Epoch 305/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0542 - val_loss: 0.0711\n",
      "Epoch 306/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0540 - val_loss: 0.0733\n",
      "Epoch 307/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0547 - val_loss: 0.0716\n",
      "Epoch 308/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0548 - val_loss: 0.0709\n",
      "Epoch 309/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0542 - val_loss: 0.0713\n",
      "Epoch 310/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0546 - val_loss: 0.0726\n",
      "Epoch 311/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0723\n",
      "Epoch 312/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0708\n",
      "Epoch 313/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0714\n",
      "Epoch 314/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0555 - val_loss: 0.0760\n",
      "Epoch 315/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0556 - val_loss: 0.0718\n",
      "Epoch 316/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0543 - val_loss: 0.0731\n",
      "Epoch 317/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0552 - val_loss: 0.0710\n",
      "Epoch 318/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0544 - val_loss: 0.0710\n",
      "Epoch 319/1000\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.0543 - val_loss: 0.0721\n",
      "Epoch 320/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0544 - val_loss: 0.0715\n",
      "Epoch 321/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0732\n",
      "Epoch 322/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0706\n",
      "Epoch 323/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0734\n",
      "Epoch 324/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0548 - val_loss: 0.0709\n",
      "Epoch 325/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0709\n",
      "Epoch 326/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0742\n",
      "Epoch 327/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0559 - val_loss: 0.0706\n",
      "Epoch 328/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0541 - val_loss: 0.0732\n",
      "Epoch 329/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0551 - val_loss: 0.0752\n",
      "Epoch 330/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0552 - val_loss: 0.0718\n",
      "Epoch 331/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0544 - val_loss: 0.0740\n",
      "Epoch 332/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0551 - val_loss: 0.0765\n",
      "Epoch 333/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0556 - val_loss: 0.0712\n",
      "Epoch 334/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0544 - val_loss: 0.0720\n",
      "Epoch 335/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0546 - val_loss: 0.0710\n",
      "Epoch 336/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0542 - val_loss: 0.0734\n",
      "Epoch 337/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0736\n",
      "Epoch 338/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0553 - val_loss: 0.0705\n",
      "Epoch 339/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0718\n",
      "Epoch 340/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0547 - val_loss: 0.0731\n",
      "Epoch 341/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0545 - val_loss: 0.0712\n",
      "Epoch 342/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0543 - val_loss: 0.0766\n",
      "Epoch 343/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0552 - val_loss: 0.0717\n",
      "Epoch 344/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0551 - val_loss: 0.0706\n",
      "Epoch 345/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0548 - val_loss: 0.0713\n",
      "Epoch 346/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0547 - val_loss: 0.0724\n",
      "Epoch 347/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0718\n",
      "Epoch 348/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0740\n",
      "Epoch 349/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0715\n",
      "Epoch 350/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0721\n",
      "Epoch 351/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0722\n",
      "Epoch 352/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0545 - val_loss: 0.0722\n",
      "Epoch 353/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0721\n",
      "Epoch 354/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0715\n",
      "Epoch 355/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0711\n",
      "Epoch 356/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0548 - val_loss: 0.0722\n",
      "Epoch 357/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0542 - val_loss: 0.0738\n",
      "Epoch 358/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0546 - val_loss: 0.0709\n",
      "Epoch 359/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0544 - val_loss: 0.0731\n",
      "Epoch 360/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0557 - val_loss: 0.0705\n",
      "Epoch 361/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0542 - val_loss: 0.0730\n",
      "Epoch 362/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0723\n",
      "Epoch 363/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0554 - val_loss: 0.0713\n",
      "Epoch 364/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0710\n",
      "Epoch 365/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0547 - val_loss: 0.0714\n",
      "Epoch 366/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0544 - val_loss: 0.0722\n",
      "Epoch 367/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0720\n",
      "Epoch 368/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0729\n",
      "Epoch 369/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0708\n",
      "Epoch 370/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0541 - val_loss: 0.0790\n",
      "Epoch 371/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0561 - val_loss: 0.0705\n",
      "Epoch 372/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0544 - val_loss: 0.0708\n",
      "Epoch 373/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0550 - val_loss: 0.0715\n",
      "Epoch 374/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0742\n",
      "Epoch 375/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0549 - val_loss: 0.0730\n",
      "Epoch 376/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0551 - val_loss: 0.0720\n",
      "Epoch 377/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0547 - val_loss: 0.0710\n",
      "Epoch 378/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0717\n",
      "Epoch 379/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0548 - val_loss: 0.0716\n",
      "Epoch 380/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0543 - val_loss: 0.0880\n",
      "Epoch 381/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0580 - val_loss: 0.0717\n",
      "Epoch 382/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0547 - val_loss: 0.0723\n",
      "Epoch 383/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0556 - val_loss: 0.0710\n",
      "Epoch 384/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0544 - val_loss: 0.0724\n",
      "Epoch 385/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0547 - val_loss: 0.0726\n",
      "Epoch 386/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0545 - val_loss: 0.0759\n",
      "Epoch 387/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0555 - val_loss: 0.0708\n",
      "Epoch 388/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0548 - val_loss: 0.0722\n",
      "Epoch 389/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0550 - val_loss: 0.0709\n",
      "Epoch 390/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0547 - val_loss: 0.0712\n",
      "Epoch 391/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0543 - val_loss: 0.0733\n",
      "Epoch 392/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0551 - val_loss: 0.0728\n",
      "Epoch 393/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0549 - val_loss: 0.0730\n",
      "Epoch 394/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0551 - val_loss: 0.0720\n",
      "Epoch 395/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0731\n",
      "Epoch 396/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0709\n",
      "Epoch 397/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0550 - val_loss: 0.0708\n",
      "Epoch 398/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0714\n",
      "Epoch 399/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0549 - val_loss: 0.0711\n",
      "Epoch 400/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0727\n",
      "\n",
      "Epoch 00400: saving model to ./log_weights/Ide_AE_weights.0400.hdf5\n",
      "Epoch 401/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0554 - val_loss: 0.0717\n",
      "Epoch 402/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0550 - val_loss: 0.0719\n",
      "Epoch 403/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0546 - val_loss: 0.0709\n",
      "Epoch 404/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0545 - val_loss: 0.0714\n",
      "Epoch 405/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0713\n",
      "Epoch 406/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0719\n",
      "Epoch 407/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0547 - val_loss: 0.0712\n",
      "Epoch 408/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0550 - val_loss: 0.0714\n",
      "Epoch 409/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0759\n",
      "Epoch 410/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0554 - val_loss: 0.0725\n",
      "Epoch 411/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0760\n",
      "Epoch 412/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0565 - val_loss: 0.0715\n",
      "Epoch 413/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0547 - val_loss: 0.0792\n",
      "Epoch 414/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0567 - val_loss: 0.0714\n",
      "Epoch 415/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0546 - val_loss: 0.0751\n",
      "Epoch 416/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0557 - val_loss: 0.0709\n",
      "Epoch 417/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0546 - val_loss: 0.0714\n",
      "Epoch 418/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0766\n",
      "Epoch 419/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0553 - val_loss: 0.0724\n",
      "Epoch 420/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0717\n",
      "Epoch 421/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0736\n",
      "Epoch 422/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0723\n",
      "Epoch 423/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0722\n",
      "Epoch 424/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0547 - val_loss: 0.0720\n",
      "Epoch 425/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0708\n",
      "Epoch 426/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0547 - val_loss: 0.0736\n",
      "Epoch 427/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0550 - val_loss: 0.0711\n",
      "Epoch 428/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0544 - val_loss: 0.0721\n",
      "Epoch 429/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0718\n",
      "Epoch 430/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0715\n",
      "Epoch 431/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0548 - val_loss: 0.0751\n",
      "Epoch 432/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0550 - val_loss: 0.0759\n",
      "Epoch 433/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0552 - val_loss: 0.0722\n",
      "Epoch 434/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0545 - val_loss: 0.0775\n",
      "Epoch 435/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0563 - val_loss: 0.0705\n",
      "Epoch 436/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0550 - val_loss: 0.0727\n",
      "Epoch 437/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0548 - val_loss: 0.0737\n",
      "Epoch 438/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0547 - val_loss: 0.0752\n",
      "Epoch 439/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0552 - val_loss: 0.0740\n",
      "Epoch 440/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0556 - val_loss: 0.0730\n",
      "Epoch 441/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0555 - val_loss: 0.0709\n",
      "Epoch 442/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0547 - val_loss: 0.0717\n",
      "Epoch 443/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0545 - val_loss: 0.0726\n",
      "Epoch 444/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0551 - val_loss: 0.0711\n",
      "Epoch 445/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0548 - val_loss: 0.0730\n",
      "Epoch 446/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0548 - val_loss: 0.0726\n",
      "Epoch 447/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0546 - val_loss: 0.0752\n",
      "Epoch 448/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0556 - val_loss: 0.0708\n",
      "Epoch 449/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0732\n",
      "Epoch 450/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0552 - val_loss: 0.0717\n",
      "Epoch 451/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0553 - val_loss: 0.0713\n",
      "Epoch 452/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0549 - val_loss: 0.0769\n",
      "Epoch 453/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0557 - val_loss: 0.0736\n",
      "Epoch 454/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0553 - val_loss: 0.0710\n",
      "Epoch 455/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0550 - val_loss: 0.0735\n",
      "Epoch 456/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0551 - val_loss: 0.0787\n",
      "Epoch 457/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0562 - val_loss: 0.0710\n",
      "Epoch 458/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0543 - val_loss: 0.0796\n",
      "Epoch 459/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0568 - val_loss: 0.0708\n",
      "Epoch 460/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0548 - val_loss: 0.0708\n",
      "Epoch 461/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0563 - val_loss: 0.0746\n",
      "Epoch 462/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0550 - val_loss: 0.0762\n",
      "Epoch 463/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0558 - val_loss: 0.0709\n",
      "Epoch 464/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0554 - val_loss: 0.0719\n",
      "Epoch 465/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0556 - val_loss: 0.0721\n",
      "Epoch 466/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0549 - val_loss: 0.0756\n",
      "Epoch 467/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0555 - val_loss: 0.0744\n",
      "Epoch 468/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0553 - val_loss: 0.0762\n",
      "Epoch 469/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0564 - val_loss: 0.0714\n",
      "Epoch 470/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0553 - val_loss: 0.0824\n",
      "Epoch 471/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0569 - val_loss: 0.0755\n",
      "Epoch 472/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0556 - val_loss: 0.0728\n",
      "Epoch 473/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0558 - val_loss: 0.0709\n",
      "Epoch 474/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0551 - val_loss: 0.0742\n",
      "Epoch 475/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0563 - val_loss: 0.0728\n",
      "Epoch 476/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0554 - val_loss: 0.0724\n",
      "Epoch 477/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0548 - val_loss: 0.0811\n",
      "Epoch 478/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0568 - val_loss: 0.0741\n",
      "Epoch 479/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0558 - val_loss: 0.0743\n",
      "Epoch 480/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0556 - val_loss: 0.0835\n",
      "Epoch 481/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0578 - val_loss: 0.0708\n",
      "Epoch 482/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0550 - val_loss: 0.0728\n",
      "Epoch 483/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0561 - val_loss: 0.0718\n",
      "Epoch 484/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0551 - val_loss: 0.0718\n",
      "Epoch 485/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0548 - val_loss: 0.0777\n",
      "Epoch 486/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0558 - val_loss: 0.0802\n",
      "Epoch 487/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0569 - val_loss: 0.0712\n",
      "Epoch 488/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0552 - val_loss: 0.0718\n",
      "Epoch 489/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0554 - val_loss: 0.0744\n",
      "Epoch 490/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0553 - val_loss: 0.0748\n",
      "Epoch 491/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0558 - val_loss: 0.0731\n",
      "Epoch 492/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0560 - val_loss: 0.0713\n",
      "Epoch 493/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0550 - val_loss: 0.0800\n",
      "Epoch 494/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0570 - val_loss: 0.0712\n",
      "Epoch 495/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0552 - val_loss: 0.0708\n",
      "Epoch 496/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0555 - val_loss: 0.0762\n",
      "Epoch 497/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0566 - val_loss: 0.0724\n",
      "Epoch 498/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0549 - val_loss: 0.0789\n",
      "Epoch 499/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0569 - val_loss: 0.0708\n",
      "Epoch 500/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0554 - val_loss: 0.0722\n",
      "\n",
      "Epoch 00500: saving model to ./log_weights/Ide_AE_weights.0500.hdf5\n",
      "Epoch 501/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0563 - val_loss: 0.0798\n",
      "Epoch 502/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0570 - val_loss: 0.0726\n",
      "Epoch 503/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0557 - val_loss: 0.0708\n",
      "Epoch 504/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0564 - val_loss: 0.0729\n",
      "Epoch 505/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0554 - val_loss: 0.0733\n",
      "Epoch 506/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0555 - val_loss: 0.0725\n",
      "Epoch 507/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0563 - val_loss: 0.0785\n",
      "Epoch 508/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0564 - val_loss: 0.0740\n",
      "Epoch 509/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0554 - val_loss: 0.0772\n",
      "Epoch 510/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0567 - val_loss: 0.0713\n",
      "Epoch 511/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0558 - val_loss: 0.0737\n",
      "Epoch 512/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0557 - val_loss: 0.0727\n",
      "Epoch 513/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0556 - val_loss: 0.0751\n",
      "Epoch 514/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0556 - val_loss: 0.0791\n",
      "Epoch 515/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0578 - val_loss: 0.0781\n",
      "Epoch 516/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0575 - val_loss: 0.0716\n",
      "Epoch 517/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0559 - val_loss: 0.0727\n",
      "Epoch 518/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0563 - val_loss: 0.0724\n",
      "Epoch 519/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0557 - val_loss: 0.0711\n",
      "Epoch 520/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0579 - val_loss: 0.0719\n",
      "Epoch 521/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0558 - val_loss: 0.0815\n",
      "Epoch 522/1000\n",
      "72/72 [==============================] - 3s 40ms/step - loss: 0.0577 - val_loss: 0.0724\n",
      "Epoch 523/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0562 - val_loss: 0.0716\n",
      "Epoch 524/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0564 - val_loss: 0.0821\n",
      "Epoch 525/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0577 - val_loss: 0.0723\n",
      "Epoch 526/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0572 - val_loss: 0.0717\n",
      "Epoch 527/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0572 - val_loss: 0.0750\n",
      "Epoch 528/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0562 - val_loss: 0.0734\n",
      "Epoch 529/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0561 - val_loss: 0.0754\n",
      "Epoch 530/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0574 - val_loss: 0.0732\n",
      "Epoch 531/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0568 - val_loss: 0.0876\n",
      "Epoch 532/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0592 - val_loss: 0.0737\n",
      "Epoch 533/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0571 - val_loss: 0.0714\n",
      "Epoch 534/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0562 - val_loss: 0.0844\n",
      "Epoch 535/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0588 - val_loss: 0.0805\n",
      "Epoch 536/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0579 - val_loss: 0.0709\n",
      "Epoch 537/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0565 - val_loss: 0.0878\n",
      "Epoch 538/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0597 - val_loss: 0.0710\n",
      "Epoch 539/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0578 - val_loss: 0.0738\n",
      "Epoch 540/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0590 - val_loss: 0.0714\n",
      "Epoch 541/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0579 - val_loss: 0.0726\n",
      "Epoch 542/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0564 - val_loss: 0.0918\n",
      "Epoch 543/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0608 - val_loss: 0.0754\n",
      "Epoch 544/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0584 - val_loss: 0.0710\n",
      "Epoch 545/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0573 - val_loss: 0.0845\n",
      "Epoch 546/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0605 - val_loss: 0.0712\n",
      "Epoch 547/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0572 - val_loss: 0.0845\n",
      "Epoch 548/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0604 - val_loss: 0.0722\n",
      "Epoch 549/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0586 - val_loss: 0.0715\n",
      "Epoch 550/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0575 - val_loss: 0.1032\n",
      "Epoch 551/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0649 - val_loss: 0.0725\n",
      "Epoch 552/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0597 - val_loss: 0.0717\n",
      "Epoch 553/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0599 - val_loss: 0.0736\n",
      "Epoch 554/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0620 - val_loss: 0.0754\n",
      "Epoch 555/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0605 - val_loss: 0.0826\n",
      "Epoch 556/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0640 - val_loss: 0.0744\n",
      "Epoch 557/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0612 - val_loss: 0.0758\n",
      "Epoch 558/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0627 - val_loss: 0.0758\n",
      "Epoch 559/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0652 - val_loss: 0.0771\n",
      "Epoch 560/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0625 - val_loss: 0.0752\n",
      "Epoch 561/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0608 - val_loss: 0.0832\n",
      "Epoch 562/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0649 - val_loss: 0.0722\n",
      "Epoch 563/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0644 - val_loss: 0.1132\n",
      "Epoch 564/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0718 - val_loss: 0.1054\n",
      "Epoch 565/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0699 - val_loss: 0.0745\n",
      "Epoch 566/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0654 - val_loss: 0.1543\n",
      "Epoch 567/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0886 - val_loss: 0.0797\n",
      "Epoch 568/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0672 - val_loss: 0.0708\n",
      "Epoch 569/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0655 - val_loss: 0.2082\n",
      "Epoch 570/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0907 - val_loss: 0.0725\n",
      "Epoch 571/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0839 - val_loss: 0.0757\n",
      "Epoch 572/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0658 - val_loss: 0.1464\n",
      "Epoch 573/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.1002 - val_loss: 0.0715\n",
      "Epoch 574/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0619 - val_loss: 0.1092\n",
      "Epoch 575/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0716 - val_loss: 0.0758\n",
      "Epoch 576/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0616 - val_loss: 0.0964\n",
      "Epoch 577/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0608 - val_loss: 0.0756\n",
      "Epoch 578/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0630 - val_loss: 0.1506\n",
      "Epoch 579/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0695 - val_loss: 0.0760\n",
      "Epoch 580/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0564 - val_loss: 0.0633\n",
      "Epoch 581/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.1080 - val_loss: 0.0635\n",
      "Epoch 582/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.1127 - val_loss: 0.0624\n",
      "Epoch 583/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.1027 - val_loss: 0.0726\n",
      "Epoch 584/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0751 - val_loss: 0.0928\n",
      "Epoch 585/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0599 - val_loss: 0.0637\n",
      "Epoch 586/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0584 - val_loss: 0.0662\n",
      "Epoch 587/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0607 - val_loss: 0.0685\n",
      "Epoch 588/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0560 - val_loss: 0.0805\n",
      "Epoch 589/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.1833 - val_loss: 0.0685\n",
      "Epoch 590/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0642 - val_loss: 0.0743\n",
      "Epoch 591/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0634 - val_loss: 0.0862\n",
      "Epoch 592/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0598 - val_loss: 0.0714\n",
      "Epoch 593/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0542 - val_loss: 0.0638\n",
      "Epoch 594/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0584 - val_loss: 0.0629\n",
      "Epoch 595/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0686 - val_loss: 0.0620\n",
      "Epoch 596/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0537 - val_loss: 0.0625\n",
      "Epoch 597/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0654 - val_loss: 0.0677\n",
      "Epoch 598/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0895 - val_loss: 0.0851\n",
      "Epoch 599/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0517 - val_loss: 0.0694\n",
      "Epoch 600/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0615 - val_loss: 0.1013\n",
      "\n",
      "Epoch 00600: saving model to ./log_weights/Ide_AE_weights.0600.hdf5\n",
      "Epoch 601/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0561 - val_loss: 0.0602\n",
      "Epoch 602/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0462 - val_loss: 0.0686\n",
      "Epoch 603/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0567 - val_loss: 0.0646\n",
      "Epoch 604/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0466 - val_loss: 0.0687\n",
      "Epoch 605/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0588 - val_loss: 0.2387\n",
      "Epoch 606/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.1001 - val_loss: 0.0761\n",
      "Epoch 607/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0488 - val_loss: 0.0597\n",
      "Epoch 608/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0482 - val_loss: 0.0638\n",
      "Epoch 609/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0658 - val_loss: 0.0597\n",
      "Epoch 610/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0482 - val_loss: 0.0587\n",
      "Epoch 611/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0465 - val_loss: 0.0603\n",
      "Epoch 612/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0481 - val_loss: 0.0610\n",
      "Epoch 613/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0737 - val_loss: 0.0622\n",
      "Epoch 614/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0508 - val_loss: 0.0719\n",
      "Epoch 615/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0478 - val_loss: 0.0686\n",
      "Epoch 616/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0475 - val_loss: 0.0655\n",
      "Epoch 617/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0456 - val_loss: 0.0558\n",
      "Epoch 618/1000\n",
      "72/72 [==============================] - 2s 33ms/step - loss: 0.0495 - val_loss: 0.0553\n",
      "Epoch 619/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0415 - val_loss: 0.0693\n",
      "Epoch 620/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0840 - val_loss: 0.1695\n",
      "Epoch 621/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0683 - val_loss: 0.2007\n",
      "Epoch 622/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0756 - val_loss: 0.0650\n",
      "Epoch 623/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0461 - val_loss: 0.0656\n",
      "Epoch 624/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0531 - val_loss: 0.0846\n",
      "Epoch 625/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0477 - val_loss: 0.0584\n",
      "Epoch 626/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0458 - val_loss: 0.0560\n",
      "Epoch 627/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0437 - val_loss: 0.3119\n",
      "Epoch 628/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.1277 - val_loss: 0.1098\n",
      "Epoch 629/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0645 - val_loss: 0.0648\n",
      "Epoch 630/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0528 - val_loss: 0.0739\n",
      "Epoch 631/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0488 - val_loss: 0.0581\n",
      "Epoch 632/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0458 - val_loss: 0.0651\n",
      "Epoch 633/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0545 - val_loss: 0.0584\n",
      "Epoch 634/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0520 - val_loss: 0.0565\n",
      "Epoch 635/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0494 - val_loss: 0.0577\n",
      "Epoch 636/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0430 - val_loss: 0.0744\n",
      "Epoch 637/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0501 - val_loss: 0.0617\n",
      "Epoch 638/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0474 - val_loss: 0.0739\n",
      "Epoch 639/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0458 - val_loss: 0.0547\n",
      "Epoch 640/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0419 - val_loss: 0.0689\n",
      "Epoch 641/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0494 - val_loss: 0.0544\n",
      "Epoch 642/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0406 - val_loss: 0.0611\n",
      "Epoch 643/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0538 - val_loss: 0.0545\n",
      "Epoch 644/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0552 - val_loss: 0.0627\n",
      "Epoch 645/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0776 - val_loss: 0.1020\n",
      "Epoch 646/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0571 - val_loss: 0.0646\n",
      "Epoch 647/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0492 - val_loss: 0.0579\n",
      "Epoch 648/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0647 - val_loss: 0.0559\n",
      "Epoch 649/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0524 - val_loss: 0.0557\n",
      "Epoch 650/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0447 - val_loss: 0.0624\n",
      "Epoch 651/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0456 - val_loss: 0.0853\n",
      "Epoch 652/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0507 - val_loss: 0.0547\n",
      "Epoch 653/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0399 - val_loss: 0.0544\n",
      "Epoch 654/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0415 - val_loss: 0.0574\n",
      "Epoch 655/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0408 - val_loss: 0.0591\n",
      "Epoch 656/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0410 - val_loss: 0.0617\n",
      "Epoch 657/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0412 - val_loss: 0.0536\n",
      "Epoch 658/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0404 - val_loss: 0.0556\n",
      "Epoch 659/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0405 - val_loss: 0.0524\n",
      "Epoch 660/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0426 - val_loss: 0.0588\n",
      "Epoch 661/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0410 - val_loss: 0.0595\n",
      "Epoch 662/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0404 - val_loss: 0.0591\n",
      "Epoch 663/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0454 - val_loss: 0.1322\n",
      "Epoch 664/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0562 - val_loss: 0.0555\n",
      "Epoch 665/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0522 - val_loss: 0.0669\n",
      "Epoch 666/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0490 - val_loss: 0.0560\n",
      "Epoch 667/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0416 - val_loss: 0.0633\n",
      "Epoch 668/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0401 - val_loss: 0.0567\n",
      "Epoch 669/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0399 - val_loss: 0.0567\n",
      "Epoch 670/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0409 - val_loss: 0.0641\n",
      "Epoch 671/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0477 - val_loss: 0.0568\n",
      "Epoch 672/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0413 - val_loss: 0.0579\n",
      "Epoch 673/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0991 - val_loss: 0.1181\n",
      "Epoch 674/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.1007 - val_loss: 0.0712\n",
      "Epoch 675/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0507 - val_loss: 0.0630\n",
      "Epoch 676/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0441 - val_loss: 0.0583\n",
      "Epoch 677/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0414 - val_loss: 0.0552\n",
      "Epoch 678/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0397 - val_loss: 0.0664\n",
      "Epoch 679/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0463 - val_loss: 0.0564\n",
      "Epoch 680/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0401 - val_loss: 0.0614\n",
      "Epoch 681/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0401 - val_loss: 0.0643\n",
      "Epoch 682/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0415 - val_loss: 0.0542\n",
      "Epoch 683/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0392 - val_loss: 0.0552\n",
      "Epoch 684/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0446 - val_loss: 0.0586\n",
      "Epoch 685/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0500 - val_loss: 0.0549\n",
      "Epoch 686/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0401 - val_loss: 0.0554\n",
      "Epoch 687/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0409 - val_loss: 0.0602\n",
      "Epoch 688/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0436 - val_loss: 0.0900\n",
      "Epoch 689/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0651 - val_loss: 0.0800\n",
      "Epoch 690/1000\n",
      "72/72 [==============================] - 3s 41ms/step - loss: 0.0517 - val_loss: 0.0644\n",
      "Epoch 691/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0468 - val_loss: 0.1473\n",
      "Epoch 692/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0691 - val_loss: 0.0655\n",
      "Epoch 693/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0507 - val_loss: 0.0580\n",
      "Epoch 694/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0405 - val_loss: 0.0554\n",
      "Epoch 695/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0388 - val_loss: 0.0554\n",
      "Epoch 696/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0385 - val_loss: 0.0546\n",
      "Epoch 697/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0410 - val_loss: 0.0803\n",
      "Epoch 698/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0472 - val_loss: 0.0534\n",
      "Epoch 699/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0380 - val_loss: 0.0596\n",
      "Epoch 700/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0446 - val_loss: 0.0547\n",
      "\n",
      "Epoch 00700: saving model to ./log_weights/Ide_AE_weights.0700.hdf5\n",
      "Epoch 701/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0428 - val_loss: 0.0602\n",
      "Epoch 702/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0389 - val_loss: 0.0532\n",
      "Epoch 703/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0402 - val_loss: 0.0733\n",
      "Epoch 704/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0523 - val_loss: 0.2742\n",
      "Epoch 705/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.1096 - val_loss: 0.0843\n",
      "Epoch 706/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0486 - val_loss: 0.0668\n",
      "Epoch 707/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0596 - val_loss: 0.1782\n",
      "Epoch 708/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0824 - val_loss: 0.0925\n",
      "Epoch 709/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0570 - val_loss: 0.0656\n",
      "Epoch 710/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0498 - val_loss: 0.0587\n",
      "Epoch 711/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0430 - val_loss: 0.0561\n",
      "Epoch 712/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0402 - val_loss: 0.0550\n",
      "Epoch 713/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0410 - val_loss: 0.0607\n",
      "Epoch 714/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0467 - val_loss: 0.0600\n",
      "Epoch 715/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0432 - val_loss: 0.0563\n",
      "Epoch 716/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0421 - val_loss: 0.0587\n",
      "Epoch 717/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0415 - val_loss: 0.0536\n",
      "Epoch 718/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0512 - val_loss: 0.0642\n",
      "Epoch 719/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0447 - val_loss: 0.0767\n",
      "Epoch 720/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0481 - val_loss: 0.0685\n",
      "Epoch 721/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0419 - val_loss: 0.0624\n",
      "Epoch 722/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0409 - val_loss: 0.0554\n",
      "Epoch 723/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0393 - val_loss: 0.0557\n",
      "Epoch 724/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0445 - val_loss: 0.0618\n",
      "Epoch 725/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0416 - val_loss: 0.0583\n",
      "Epoch 726/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0441 - val_loss: 0.0561\n",
      "Epoch 727/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0397 - val_loss: 0.0631\n",
      "Epoch 728/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0412 - val_loss: 0.0560\n",
      "Epoch 729/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0412 - val_loss: 0.0653\n",
      "Epoch 730/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0425 - val_loss: 0.0554\n",
      "Epoch 731/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0379 - val_loss: 0.0582\n",
      "Epoch 732/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0377 - val_loss: 0.0523\n",
      "Epoch 733/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0413 - val_loss: 0.0552\n",
      "Epoch 734/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0533 - val_loss: 0.0569\n",
      "Epoch 735/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0468 - val_loss: 0.0507\n",
      "Epoch 736/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0367 - val_loss: 0.0545\n",
      "Epoch 737/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0382 - val_loss: 0.0525\n",
      "Epoch 738/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0594 - val_loss: 0.0739\n",
      "Epoch 739/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0887 - val_loss: 0.0736\n",
      "Epoch 740/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0592 - val_loss: 0.0787\n",
      "Epoch 741/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0604 - val_loss: 0.0557\n",
      "Epoch 742/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0413 - val_loss: 0.0595\n",
      "Epoch 743/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0405 - val_loss: 0.0650\n",
      "Epoch 744/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0414 - val_loss: 0.0564\n",
      "Epoch 745/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0377 - val_loss: 0.0551\n",
      "Epoch 746/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0395 - val_loss: 0.0582\n",
      "Epoch 747/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0388 - val_loss: 0.0613\n",
      "Epoch 748/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0388 - val_loss: 0.0562\n",
      "Epoch 749/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0389 - val_loss: 0.0576\n",
      "Epoch 750/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0395 - val_loss: 0.0563\n",
      "Epoch 751/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0386 - val_loss: 0.0545\n",
      "Epoch 752/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0375 - val_loss: 0.0555\n",
      "Epoch 753/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0433 - val_loss: 0.0515\n",
      "Epoch 754/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0416 - val_loss: 0.0806\n",
      "Epoch 755/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0566 - val_loss: 0.0572\n",
      "Epoch 756/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0373 - val_loss: 0.0561\n",
      "Epoch 757/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0454 - val_loss: 0.0577\n",
      "Epoch 758/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0425 - val_loss: 0.0606\n",
      "Epoch 759/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0378 - val_loss: 0.0520\n",
      "Epoch 760/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0376 - val_loss: 0.0511\n",
      "Epoch 761/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0390 - val_loss: 0.0547\n",
      "Epoch 762/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0394 - val_loss: 0.0593\n",
      "Epoch 763/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0374 - val_loss: 0.0529\n",
      "Epoch 764/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0398 - val_loss: 0.0771\n",
      "Epoch 765/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0409 - val_loss: 0.0550\n",
      "Epoch 766/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0417 - val_loss: 0.0614\n",
      "Epoch 767/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0503 - val_loss: 0.0658\n",
      "Epoch 768/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0515 - val_loss: 0.0555\n",
      "Epoch 769/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0422 - val_loss: 0.0863\n",
      "Epoch 770/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.1456 - val_loss: 0.0619\n",
      "Epoch 771/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0817 - val_loss: 0.0955\n",
      "Epoch 772/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0622 - val_loss: 0.0964\n",
      "Epoch 773/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0618 - val_loss: 0.0689\n",
      "Epoch 774/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0509 - val_loss: 0.0651\n",
      "Epoch 775/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0471 - val_loss: 0.0599\n",
      "Epoch 776/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0423 - val_loss: 0.0571\n",
      "Epoch 777/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0413 - val_loss: 0.0569\n",
      "Epoch 778/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0401 - val_loss: 0.0594\n",
      "Epoch 779/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0418 - val_loss: 0.0589\n",
      "Epoch 780/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0442 - val_loss: 0.0572\n",
      "Epoch 781/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0418 - val_loss: 0.0660\n",
      "Epoch 782/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0403 - val_loss: 0.0532\n",
      "Epoch 783/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0375 - val_loss: 0.0539\n",
      "Epoch 784/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0389 - val_loss: 0.0560\n",
      "Epoch 785/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0425 - val_loss: 0.0530\n",
      "Epoch 786/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0386 - val_loss: 0.0534\n",
      "Epoch 787/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0379 - val_loss: 0.0517\n",
      "Epoch 788/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0374 - val_loss: 0.0544\n",
      "Epoch 789/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0385 - val_loss: 0.0577\n",
      "Epoch 790/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0403 - val_loss: 0.0596\n",
      "Epoch 791/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0414 - val_loss: 0.0673\n",
      "Epoch 792/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0439 - val_loss: 0.0653\n",
      "Epoch 793/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0398 - val_loss: 0.0539\n",
      "Epoch 794/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0369 - val_loss: 0.0537\n",
      "Epoch 795/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0364 - val_loss: 0.0664\n",
      "Epoch 796/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0487 - val_loss: 0.0624\n",
      "Epoch 797/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0390 - val_loss: 0.0534\n",
      "Epoch 798/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0375 - val_loss: 0.0541\n",
      "Epoch 799/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0430 - val_loss: 0.0517\n",
      "Epoch 800/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0457 - val_loss: 0.0510\n",
      "\n",
      "Epoch 00800: saving model to ./log_weights/Ide_AE_weights.0800.hdf5\n",
      "Epoch 801/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0355 - val_loss: 0.0509\n",
      "Epoch 802/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0356 - val_loss: 0.0532\n",
      "Epoch 803/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0365 - val_loss: 0.0694\n",
      "Epoch 804/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0407 - val_loss: 0.0526\n",
      "Epoch 805/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0379 - val_loss: 0.0542\n",
      "Epoch 806/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0357 - val_loss: 0.0566\n",
      "Epoch 807/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0382 - val_loss: 0.0534\n",
      "Epoch 808/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0411 - val_loss: 0.0529\n",
      "Epoch 809/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0458 - val_loss: 0.0638\n",
      "Epoch 810/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0401 - val_loss: 0.0866\n",
      "Epoch 811/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0610 - val_loss: 0.0734\n",
      "Epoch 812/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0521 - val_loss: 0.0709\n",
      "Epoch 813/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0940 - val_loss: 0.2158\n",
      "Epoch 814/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0888 - val_loss: 0.0619\n",
      "Epoch 815/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0456 - val_loss: 0.0635\n",
      "Epoch 816/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0462 - val_loss: 0.0738\n",
      "Epoch 817/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0445 - val_loss: 0.0672\n",
      "Epoch 818/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0434 - val_loss: 0.0573\n",
      "Epoch 819/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0418 - val_loss: 0.0563\n",
      "Epoch 820/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0388 - val_loss: 0.0579\n",
      "Epoch 821/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0414 - val_loss: 0.0610\n",
      "Epoch 822/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0432 - val_loss: 0.0743\n",
      "Epoch 823/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0458 - val_loss: 0.0633\n",
      "Epoch 824/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0431 - val_loss: 0.0524\n",
      "Epoch 825/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0371 - val_loss: 0.0529\n",
      "Epoch 826/1000\n",
      "72/72 [==============================] - 2s 33ms/step - loss: 0.0382 - val_loss: 0.0548\n",
      "Epoch 827/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0373 - val_loss: 0.0552\n",
      "Epoch 828/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0370 - val_loss: 0.0550\n",
      "Epoch 829/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0370 - val_loss: 0.0521\n",
      "Epoch 830/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0368 - val_loss: 0.0587\n",
      "Epoch 831/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0393 - val_loss: 0.0527\n",
      "Epoch 832/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0392 - val_loss: 0.0542\n",
      "Epoch 833/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0390 - val_loss: 0.0528\n",
      "Epoch 834/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0366 - val_loss: 0.0585\n",
      "Epoch 835/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0373 - val_loss: 0.0552\n",
      "Epoch 836/1000\n",
      "72/72 [==============================] - 2s 33ms/step - loss: 0.0371 - val_loss: 0.0606\n",
      "Epoch 837/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0394 - val_loss: 0.0511\n",
      "Epoch 838/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0349 - val_loss: 0.0501\n",
      "Epoch 839/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0369 - val_loss: 0.0512\n",
      "Epoch 840/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0389 - val_loss: 0.0533\n",
      "Epoch 841/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0378 - val_loss: 0.0625\n",
      "Epoch 842/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0525 - val_loss: 0.0840\n",
      "Epoch 843/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0472 - val_loss: 0.0537\n",
      "Epoch 844/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0378 - val_loss: 0.0511\n",
      "Epoch 845/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0370 - val_loss: 0.0546\n",
      "Epoch 846/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0426 - val_loss: 0.0694\n",
      "Epoch 847/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0416 - val_loss: 0.0570\n",
      "Epoch 848/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0372 - val_loss: 0.0715\n",
      "Epoch 849/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0409 - val_loss: 0.0547\n",
      "Epoch 850/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0357 - val_loss: 0.0531\n",
      "Epoch 851/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0364 - val_loss: 0.0593\n",
      "Epoch 852/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0457 - val_loss: 0.0523\n",
      "Epoch 853/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0390 - val_loss: 0.0525\n",
      "Epoch 854/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0402 - val_loss: 0.0790\n",
      "Epoch 855/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0442 - val_loss: 0.0706\n",
      "Epoch 856/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0419 - val_loss: 0.0670\n",
      "Epoch 857/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0377 - val_loss: 0.0532\n",
      "Epoch 858/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0352 - val_loss: 0.0593\n",
      "Epoch 859/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0387 - val_loss: 0.0592\n",
      "Epoch 860/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0409 - val_loss: 0.0534\n",
      "Epoch 861/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0368 - val_loss: 0.0673\n",
      "Epoch 862/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0414 - val_loss: 0.0663\n",
      "Epoch 863/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0412 - val_loss: 0.0662\n",
      "Epoch 864/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0426 - val_loss: 0.0578\n",
      "Epoch 865/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0439 - val_loss: 0.0543\n",
      "Epoch 866/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0399 - val_loss: 0.0592\n",
      "Epoch 867/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0361 - val_loss: 0.0520\n",
      "Epoch 868/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0347 - val_loss: 0.0577\n",
      "Epoch 869/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0385 - val_loss: 0.0548\n",
      "Epoch 870/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0370 - val_loss: 0.0569\n",
      "Epoch 871/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0399 - val_loss: 0.0570\n",
      "Epoch 872/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0424 - val_loss: 0.0590\n",
      "Epoch 873/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0550 - val_loss: 0.1261\n",
      "Epoch 874/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0726 - val_loss: 0.0880\n",
      "Epoch 875/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0597 - val_loss: 0.0657\n",
      "Epoch 876/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0494 - val_loss: 0.0564\n",
      "Epoch 877/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0380 - val_loss: 0.0570\n",
      "Epoch 878/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0400 - val_loss: 0.0625\n",
      "Epoch 879/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0401 - val_loss: 0.0528\n",
      "Epoch 880/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0369 - val_loss: 0.0523\n",
      "Epoch 881/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0368 - val_loss: 0.0545\n",
      "Epoch 882/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0387 - val_loss: 0.0560\n",
      "Epoch 883/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0362 - val_loss: 0.0563\n",
      "Epoch 884/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0397 - val_loss: 0.0523\n",
      "Epoch 885/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0368 - val_loss: 0.0521\n",
      "Epoch 886/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0352 - val_loss: 0.0575\n",
      "Epoch 887/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0364 - val_loss: 0.0520\n",
      "Epoch 888/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0354 - val_loss: 0.0535\n",
      "Epoch 889/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0371 - val_loss: 0.0540\n",
      "Epoch 890/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0397 - val_loss: 0.0508\n",
      "Epoch 891/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0344 - val_loss: 0.0518\n",
      "Epoch 892/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0363 - val_loss: 0.0568\n",
      "Epoch 893/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0413 - val_loss: 0.0572\n",
      "Epoch 894/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0568 - val_loss: 0.0751\n",
      "Epoch 895/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0419 - val_loss: 0.0507\n",
      "Epoch 896/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0367 - val_loss: 0.0531\n",
      "Epoch 897/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0352 - val_loss: 0.0537\n",
      "Epoch 898/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0349 - val_loss: 0.0610\n",
      "Epoch 899/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0368 - val_loss: 0.0501\n",
      "Epoch 900/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0402 - val_loss: 0.0665\n",
      "\n",
      "Epoch 00900: saving model to ./log_weights/Ide_AE_weights.0900.hdf5\n",
      "Epoch 901/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0957 - val_loss: 0.0951\n",
      "Epoch 902/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0804 - val_loss: 0.0563\n",
      "Epoch 903/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0480 - val_loss: 0.0613\n",
      "Epoch 904/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0424 - val_loss: 0.0590\n",
      "Epoch 905/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0414 - val_loss: 0.0562\n",
      "Epoch 906/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0392 - val_loss: 0.0558\n",
      "Epoch 907/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0383 - val_loss: 0.0568\n",
      "Epoch 908/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0386 - val_loss: 0.0655\n",
      "Epoch 909/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0407 - val_loss: 0.0563\n",
      "Epoch 910/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0389 - val_loss: 0.0562\n",
      "Epoch 911/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0376 - val_loss: 0.0530\n",
      "Epoch 912/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0358 - val_loss: 0.0529\n",
      "Epoch 913/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0361 - val_loss: 0.0558\n",
      "Epoch 914/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0369 - val_loss: 0.0752\n",
      "Epoch 915/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0484 - val_loss: 0.0894\n",
      "Epoch 916/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0502 - val_loss: 0.0522\n",
      "Epoch 917/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0365 - val_loss: 0.0519\n",
      "Epoch 918/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0396 - val_loss: 0.0520\n",
      "Epoch 919/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0486 - val_loss: 0.0714\n",
      "Epoch 920/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0415 - val_loss: 0.0598\n",
      "Epoch 921/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0395 - val_loss: 0.0740\n",
      "Epoch 922/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0498 - val_loss: 0.0564\n",
      "Epoch 923/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0378 - val_loss: 0.0532\n",
      "Epoch 924/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0399 - val_loss: 0.0531\n",
      "Epoch 925/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0374 - val_loss: 0.0558\n",
      "Epoch 926/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0379 - val_loss: 0.0541\n",
      "Epoch 927/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0362 - val_loss: 0.0588\n",
      "Epoch 928/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0388 - val_loss: 0.0567\n",
      "Epoch 929/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0357 - val_loss: 0.0562\n",
      "Epoch 930/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0394 - val_loss: 0.0525\n",
      "Epoch 931/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0464 - val_loss: 0.0575\n",
      "Epoch 932/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0917 - val_loss: 0.0677\n",
      "Epoch 933/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0436 - val_loss: 0.0763\n",
      "Epoch 934/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0454 - val_loss: 0.0684\n",
      "Epoch 935/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0437 - val_loss: 0.0553\n",
      "Epoch 936/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0387 - val_loss: 0.0619\n",
      "Epoch 937/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0389 - val_loss: 0.0566\n",
      "Epoch 938/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0396 - val_loss: 0.0553\n",
      "Epoch 939/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0396 - val_loss: 0.0529\n",
      "Epoch 940/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0373 - val_loss: 0.0576\n",
      "Epoch 941/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0376 - val_loss: 0.0566\n",
      "Epoch 942/1000\n",
      "72/72 [==============================] - 3s 38ms/step - loss: 0.0365 - val_loss: 0.0571\n",
      "Epoch 943/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0390 - val_loss: 0.0515\n",
      "Epoch 944/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0369 - val_loss: 0.0551\n",
      "Epoch 945/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0400 - val_loss: 0.0522\n",
      "Epoch 946/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0373 - val_loss: 0.0541\n",
      "Epoch 947/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0360 - val_loss: 0.0606\n",
      "Epoch 948/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0382 - val_loss: 0.0568\n",
      "Epoch 949/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0357 - val_loss: 0.0556\n",
      "Epoch 950/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0523 - val_loss: 0.0538\n",
      "Epoch 951/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0404 - val_loss: 0.0696\n",
      "Epoch 952/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0417 - val_loss: 0.0559\n",
      "Epoch 953/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0558 - val_loss: 0.0507\n",
      "Epoch 954/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0410 - val_loss: 0.0605\n",
      "Epoch 955/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0396 - val_loss: 0.0574\n",
      "Epoch 956/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0387 - val_loss: 0.0543\n",
      "Epoch 957/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0363 - val_loss: 0.0582\n",
      "Epoch 958/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0360 - val_loss: 0.0523\n",
      "Epoch 959/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0363 - val_loss: 0.0507\n",
      "Epoch 960/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0363 - val_loss: 0.0728\n",
      "Epoch 961/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0407 - val_loss: 0.0528\n",
      "Epoch 962/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0346 - val_loss: 0.0504\n",
      "Epoch 963/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0365 - val_loss: 0.0584\n",
      "Epoch 964/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0390 - val_loss: 0.0536\n",
      "Epoch 965/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0400 - val_loss: 0.0512\n",
      "Epoch 966/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0364 - val_loss: 0.0504\n",
      "Epoch 967/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0354 - val_loss: 0.0570\n",
      "Epoch 968/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0375 - val_loss: 0.0511\n",
      "Epoch 969/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0369 - val_loss: 0.0541\n",
      "Epoch 970/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0384 - val_loss: 0.0512\n",
      "Epoch 971/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0344 - val_loss: 0.0550\n",
      "Epoch 972/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0384 - val_loss: 0.0571\n",
      "Epoch 973/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0422 - val_loss: 0.0590\n",
      "Epoch 974/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0386 - val_loss: 0.0567\n",
      "Epoch 975/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0357 - val_loss: 0.0544\n",
      "Epoch 976/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0374 - val_loss: 0.0586\n",
      "Epoch 977/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0354 - val_loss: 0.0533\n",
      "Epoch 978/1000\n",
      "72/72 [==============================] - 3s 39ms/step - loss: 0.0354 - val_loss: 0.0514\n",
      "Epoch 979/1000\n",
      "72/72 [==============================] - 3s 37ms/step - loss: 0.0383 - val_loss: 0.0526\n",
      "Epoch 980/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0377 - val_loss: 0.0506\n",
      "Epoch 981/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0500 - val_loss: 0.0810\n",
      "Epoch 982/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0524 - val_loss: 0.0543\n",
      "Epoch 983/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0389 - val_loss: 0.0512\n",
      "Epoch 984/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0356 - val_loss: 0.0518\n",
      "Epoch 985/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0350 - val_loss: 0.0586\n",
      "Epoch 986/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0376 - val_loss: 0.0526\n",
      "Epoch 987/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0367 - val_loss: 0.0529\n",
      "Epoch 988/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0360 - val_loss: 0.0513\n",
      "Epoch 989/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0378 - val_loss: 0.0503\n",
      "Epoch 990/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0404 - val_loss: 0.0502\n",
      "Epoch 991/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0369 - val_loss: 0.0610\n",
      "Epoch 992/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0378 - val_loss: 0.0671\n",
      "Epoch 993/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0399 - val_loss: 0.0563\n",
      "Epoch 994/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0363 - val_loss: 0.0620\n",
      "Epoch 995/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0370 - val_loss: 0.0492\n",
      "Epoch 996/1000\n",
      "72/72 [==============================] - 3s 36ms/step - loss: 0.0367 - val_loss: 0.0563\n",
      "Epoch 997/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0425 - val_loss: 0.0811\n",
      "Epoch 998/1000\n",
      "72/72 [==============================] - 2s 34ms/step - loss: 0.0506 - val_loss: 0.0977\n",
      "Epoch 999/1000\n",
      "72/72 [==============================] - 3s 35ms/step - loss: 0.0530 - val_loss: 0.0565\n",
      "Epoch 1000/1000\n",
      "72/72 [==============================] - 2s 35ms/step - loss: 0.0389 - val_loss: 0.0557\n",
      "\n",
      "Epoch 01000: saving model to ./log_weights/Ide_AE_weights.1000.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint=ModelCheckpoint('./log_weights/Ide_AE_weights.{epoch:04d}.hdf5',period=100,save_weights_only=True,verbose=1)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(Ide_AE.layers[1].get_weights()))\n",
    "\n",
    "Ide_AE_history = Ide_AE.fit(x_train, x_train,\\\n",
    "                            epochs=epochs_number,\\\n",
    "                            batch_size=batch_size_value,\\\n",
    "                            shuffle=True,\\\n",
    "                            validation_data=(x_validate,x_validate),\\\n",
    "                            callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = Ide_AE_history.history['loss']\n",
    "val_loss = Ide_AE_history.history['val_loss']\n",
    "\n",
    "epochs = range(epochs_number)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for one-to-one map layer 0.03514596940249229\n"
     ]
    }
   ],
   "source": [
    "p_data=Ide_AE.predict(x_test)\n",
    "numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)-x_test,2))/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "key_number=64\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_number=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features=F.top_k_keepWeights_1(Ide_AE.get_layer(index=1).get_weights()[0],key_number)\n",
    "\n",
    "selected_position_list=np.where(key_features>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 1.0\n",
      "Training accuracy 1.0\n",
      "Testing accuracy 0.9523809523809523\n",
      "Testing accuracy 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "train_feature=C_train_x\n",
    "train_label=C_train_y\n",
    "test_feature=C_test_x\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 64)\n",
      "(21, 64)\n",
      "Training accuracy 1.0\n",
      "Training accuracy 1.0\n",
      "Testing accuracy 0.7619047619047619\n",
      "Testing accuracy 0.7619047619047619\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 64)\n",
      "(21, 64)\n",
      "0.3867693678875625\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(C_train_selected_x.shape)\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(C_test_selected_x.shape)\n",
    "\n",
    "\n",
    "train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
